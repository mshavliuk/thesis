@online{AchievingKanonymityPrivacy,
  title = {Achieving K-Anonymity Privacy Protection Using Generalization and Suppression | {{International Journal}} of {{Uncertainty}}, {{Fuzziness}} and {{Knowledge-Based Systems}}},
  url = {https://dl.acm.org/doi/10.1142/S021848850200165X},
  urldate = {2024-11-14},
  file = {/Users/mikhail/Zotero/storage/T46YLHCI/Achieving k-anonymity privacy protection using generalization and suppression  International Journa.pdf;/Users/mikhail/Zotero/storage/SEQPMPPL/S021848850200165X.html}
}

@online{ApacheSpark,
  title = {Apache {{Spark}}™ - {{Unified Engine}} for Large-Scale Data Analytics},
  url = {https://spark.apache.org/},
  urldate = {2024-07-30},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/FA4C4KPF/spark.apache.org.html}
}

@online{AttentionAllYouNeed2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017},
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  urldate = {2024-06-17},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  version = {7},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),notion},
  file = {/Users/mikhail/Zotero/storage/AXQI45U6/Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@article{AUTOMATEDRISKPREDICTION2022,
  title = {{{AUTOMATED RISK PREDICTION FROM HEALTH DATA}}},
  author = {Tommola, Janne},
  date = {2022},
  abstract = {This work investigates the prediction of diagnoses to patients based on their electronic health records (EHRs). The work compares traditional risk calculators based on scientific research with newer neural networks utilizing machine learning. The material used in the work is the US-based MIMIC-III patient database, and part of the work is determining the suitability of it for risk prediction. MIMIC-III is an intensive care database of 46,520 patients that is available for research use under certain conditions. One of the goals of the work is to prepare for the prediction of diagnoses using the data in the Finnish Kanta patient database. The research questions of this work are 1) whether it is feasible to utilize conventional risk models on EHRs, and 2) how machine learning methods compare to the conventional models. Part of the work is determining the availability and validity of the input variables needed in the risk calculators, and utilizing the less structured text of the patient report. The text of the patient report is used in a BERT-based neural network model that processes natural language to determine whether the patient smokes. A finding of the study is that most traditional risk calculators cannot be used for the majority of patients due to the lack of necessary data in their EHRs, such as the family history of various cardiovascular diseases. The neural networks used in the work only need the patient’s ICD-9 diagnostic code history and basic demographic information as inputs, so they can be used for any patient. The work compares the predictive power of different methods for two diagnoses: heart failure and stroke. The risk calculators had poor predictive power, but neural networks were able to predict a future diagnosis of heart failure to some extent. None of the methods tested succeeded in predicting the diagnosis of stroke, which is rarer than heart failure. Finally, the work considers the suitability of the methods used for health care, such as population screening as-is. With further improvements, the methods could possibly also be used at the individual level to warn of risks or to assist the doctor. On the other hand, regulations and ethical challenges may limit the use of neural networks due to e.g., their more difficult interpretation, predictability and unclear liability for errors. Areas of required developments for further improvement are considered last, such as the needs for EHRs and potential changes or additions that may improve the accuracy of the models.},
  langid = {english},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/567EBFL7/Tommola - AUTOMATED RISK PREDICTION FROM HEALTH DATA.pdf}
}

@online{BagTricksImage2018,
  title = {Bag of {{Tricks}} for {{Image Classification}} with {{Convolutional Neural Networks}}},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  date = {2018-12-04},
  url = {https://arxiv.org/abs/1812.01187v2},
  urldate = {2024-09-30},
  abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
  langid = {english},
  organization = {arXiv.org},
  file = {/Users/mikhail/Zotero/storage/SKJUZSWH/He et al. - 2018 - Bag of Tricks for Image Classification with Convolutional Neural Networks.pdf}
}

@article{bashath2022data,
  title = {A Data-Centric Review of Deep Transfer Learning with Applications to Text Data},
  author = {Bashath, Samar and Perera, Nadeesha and Tripathi, Shailesh and Manjang, Kalifa and Dehmer, Matthias and Streib, Frank Emmert},
  date = {2022},
  journaltitle = {Information Sciences},
  volume = {585},
  pages = {498--528},
  publisher = {Elsevier}
}

@article{BEHRT2020,
  title = {{{BEHRT}}: {{Transformer}} for {{Electronic Health Records}}},
  shorttitle = {{{BEHRT}}},
  author = {Li, Yikuan and Rao, Shishir and Solares, José Roberto Ayala and Hassaine, Abdelaali and Ramakrishnan, Rema and Canoy, Dexter and Zhu, Yajie and Rahimi, Kazem and Salimi-Khorshidi, Gholamreza},
  date = {2020-04-28},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {7155},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-62922-y},
  url = {https://www.nature.com/articles/s41598-020-62922-y},
  urldate = {2024-06-26},
  abstract = {Abstract             Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (including deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for electronic health records (EHR), capable of simultaneously predicting the likelihood of 301 conditions in one’s future visits. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking improvement of 8.0–13.2\% (in terms of average precision scores for different tasks), over the existing state-of-the-art deep EHR models. In addition to its scalability and superior accuracy, BEHRT enables personalised interpretation of its predictions; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to further improve the accuracy of its predictions; its (pre-)training results in disease and patient representations can be useful for future studies (i.e., transfer learning).},
  langid = {english},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/NXEC9X4Q/Li et al. - 2020 - BEHRT Transformer for Electronic Health Records.pdf}
}

@article{BRLTM2021,
  title = {Bidirectional {{Representation Learning From Transformers Using Multimodal Electronic Health Record Data}} to {{Predict Depression}}},
  author = {Meng, Yiwen and Speier, William and Ong, Michael K. and Arnold, Corey W.},
  date = {2021-08},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  shortjournal = {IEEE J. Biomed. Health Inform.},
  volume = {25},
  number = {8},
  pages = {3121--3129},
  issn = {2168-2194, 2168-2208},
  doi = {10.1109/JBHI.2021.3063721},
  url = {https://ieeexplore.ieee.org/document/9369833/},
  urldate = {2024-06-19},
  abstract = {Advancements in machine learning algorithms have had a beneficial impact on representation learning, classification, and prediction models built using electronic health record (EHR) data. Effort has been put both on increasing models’ overall performance as well as improving their interpretability, particularly regarding the decision-making process. In this study, we present a temporal deep learning model to perform bidirectional representation learning on EHR sequences with a transformer architecture to predict future diagnosis of depression. This model is able to aggregate five heterogenous and highdimensional data sources from the EHR and process them in a temporal manner for chronic disease prediction at various prediction windows. We applied the current trend of pretraining and fine-tuning on EHR data to outperform the current state-of-the-art in chronic disease prediction, and to demonstrate the underlying relation between EHR codes in the sequence. The model generated the highest increases of precision-recall area under the curve (PRAUC) from 0.70 to 0.76 in depression prediction compared to the best baseline model. Furthermore, the self-attention weights in each sequence quantitatively demonstrated the inner relationship between various codes, which improved the model’s interpretability. These results demonstrate the model’s ability to utilize heterogeneous EHR data to predict depression while achieving high accuracy and interpretability, which may facilitate constructing clinical decision support systems in the future for chronic disease screening and early detection.},
  langid = {english},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/CQIU4HDL/Meng et al. - 2021 - Bidirectional Representation Learning From Transfo.pdf}
}

@online{BYOL2020,
  title = {Bootstrap Your Own Latent: {{A}} New Approach to Self-Supervised {{Learning}}},
  shorttitle = {Bootstrap Your Own Latent},
  author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
  date = {2020},
  doi = {10.48550/ARXIV.2006.07733},
  url = {https://arxiv.org/abs/2006.07733},
  urldate = {2024-06-24},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),notion},
  file = {/Users/mikhail/Zotero/storage/TCB27FLB/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf}
}

@article{ChoiceScalingTechnique2023,
  title = {The Choice of Scaling Technique Matters for Classification Performance},
  author = {family=Amorim, given=Lucas B. V., prefix=de, useprefix=true and Cavalcanti, George D. C. and Cruz, Rafael M. O.},
  date = {2023-01-01},
  journaltitle = {Applied Soft Computing},
  shortjournal = {Applied Soft Computing},
  volume = {133},
  pages = {109924},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2022.109924},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494622009735},
  urldate = {2024-11-06},
  abstract = {Dataset scaling, also known as normalization, is an essential preprocessing step in a machine learning pipeline. It is aimed at adjusting attributes scales in a way that they all vary within the same range. This transformation is known to improve the performance of classification models, but there are several scaling techniques to choose from, and this choice is not generally done carefully. In this paper, we execute a broad experiment comparing the impact of 5 scaling techniques on the performances of 20 classification algorithms among monolithic and ensemble models, applying them to 82 publicly available datasets with varying imbalance ratios. Results show that the choice of scaling technique matters for classification performance, and the performance difference between the best and the worst scaling technique is relevant and statistically significant in most cases. They also indicate that choosing an inadequate technique can be more detrimental to classification performance than not scaling the data at all. We also show how the performance variation of an ensemble model, considering different scaling techniques, tends to be dictated by that of its base model. Finally, we discuss the relationship between a model’s sensitivity to the choice of scaling technique and its performance and provide insights into its applicability on different model deployment scenarios. Full results and source code for the experiments in this paper are available in a GitHub repository.11https://github.com/amorimlb/scaling\_matters.},
  keywords = {Classification,Ensemble of classifiers,Multiple Classifier System,Normalization,Preprocessing,Scaling,Standardization},
  file = {/Users/mikhail/Zotero/storage/J2IBW7FG/de Amorim et al. - 2023 - The choice of scaling technique matters for classification performance.pdf;/Users/mikhail/Zotero/storage/R8MEW3ZV/S1568494622009735.html}
}

@online{ComprehensiveSurveyApplications2023,
  title = {A {{Comprehensive Survey}} on {{Applications}} of {{Transformers}} for {{Deep Learning Tasks}}},
  author = {Islam, Saidul and Elmekki, Hanae and Elsebai, Ahmed and Bentahar, Jamal and Drawel, Najat and Rjoub, Gaith and Pedrycz, Witold},
  date = {2023},
  doi = {10.48550/ARXIV.2306.07303},
  url = {https://arxiv.org/abs/2306.07303},
  urldate = {2024-05-19},
  abstract = {Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant absence of a comprehensive survey paper encompassing its major applications across various domains. Therefore, we undertook the task of filling this gap by conducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses the identification of the top five application domains for transformer-based models, namely: NLP, Computer Vision, Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze the impact of highly influential transformer-based models in these domains and subsequently classify them based on their respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future possibilities of transformers for enthusiastic researchers, thus contributing to the broader understanding of this groundbreaking technology.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),notion},
  file = {/Users/mikhail/Zotero/storage/QXLLF7Z6/Islam et al. - 2023 - A Comprehensive Survey on Applications of Transfor.pdf}
}

@article{CrossattentionTransformerEncoder2023,
  title = {A Cross-Attention Transformer Encoder for Paired Sequence Data},
  author = {{Ceder Dens} and {K. Laukens} and {P. Meysman} and {Wout Bittremieux}},
  date = {2023-12-12},
  journaltitle = {bioRxiv},
  shortjournal = {bioRxiv},
  doi = {10.1101/2023.12.11.571066},
  url = {https://consensus.app/papers/crossattention-transformer-encoder-paired-sequence-data-dens/bb3558497e275456a5b6a0733b0f63ea/},
  abstract = {Transformer-based sequence encoding architectures are often limited to a single-sequence input while some tasks require a multi-sequence input. For example, the peptide–MHCII binding prediction task where the input consists of two protein sequences. Current workarounds to solve this input-type mismatch lack resemblance with the biological mechanisms behind the task. As a solution, we propose a novel cross-attention transformer encoder that creates a cross-attended embedding of both input sequences. We compare its classification performance on the peptide–MHCII binding prediction task to a baseline logistic regression model and a default transformer encoder. Finally, we make visualizations of the attention layers to show how the different models learn different patterns.},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/U3VSBXBT/Ceder Dens et al. - 2023 - A cross-attention transformer encoder for paired s.pdf}
}

@inproceedings{CurriculumLearning,
  title = {Curriculum Learning},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
  date = {2009},
  series = {Icml '09},
  pages = {41--48},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1553374.1553380},
  url = {https://doi.org/10.1145/1553374.1553380},
  abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  isbn = {978-1-60558-516-1},
  pagetotal = {8}
}

@inproceedings{DataNormalizationTask2022,
  title = {Towards {{Data Normalization Task}} for the {{Efficient Mining}} of {{Medical Data}}},
  booktitle = {2022 12th {{International Conference}} on {{Advanced Computer Information Technologies}} ({{ACIT}})},
  author = {Izonin, Ivan and Ilchyshyn, Bohdan and Tkachenko, Roman and Greguš, Michal and Shakhovska, Natalya and Strauss, Christine},
  date = {2022-09},
  pages = {480--484},
  issn = {2770-5226},
  doi = {10.1109/ACIT54803.2022.9913112},
  url = {https://ieeexplore.ieee.org/document/9913112},
  urldate = {2024-11-06},
  abstract = {The paper investigates the problem of data normalization in solving medical diagnostics tasks by machine learning algorithms. The authors describe five different data normalization methods’ operations, advantages, and disadvantages. The effectiveness of their work was evaluated using two data sets with different Imbalanced Ratio, which is typical for medical tasks. The modeling was performed by solving a binary classification task using three different machine learning methods based on decision trees. It is experimentally established that the method of normalization ScalerOnCircle, unlike others, increases the efficiency of analyzing medical data based on researched machine learning methods. There was a significant increase in the F1-score value when using this normalization method. It is because ScalerOnCircle, in addition to normalization by columns, provides the possibility of considering relationships between the attributes of each vector of a given dataset. This problem is very acute in the medical field, where data sets designed for intellectual analysis are characterized by many attributes and complex nonlinear relationships between them. This fact must be taken into account when mining such datasets. ScalerOnCircle opens up several benefits for the efficient mining of medical data.},
  eventtitle = {2022 12th {{International Conference}} on {{Advanced Computer Information Technologies}} ({{ACIT}})},
  keywords = {binary classification,Data mining,data normalization,data preprocessing,Data preprocessing,decision trees,Decision trees,imbalanced ratio,machine learning,Machine learning,Machine learning algorithms,Medical diagnosis,medical diagnostics,small data,Task analysis},
  file = {/Users/mikhail/Zotero/storage/JMT4VUZ6/Izonin et al. - 2022 - Towards Data Normalization Task for the Efficient Mining of Medical Data.pdf;/Users/mikhail/Zotero/storage/RDTDSLQL/9913112.html}
}

@online{DecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2024-09-30},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/mikhail/Zotero/storage/ER3ZDND3/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Users/mikhail/Zotero/storage/UV6L7DK3/1711.html}
}

@article{DeepLearningElectronic2020,
  title = {Deep Learning for Electronic Health Records: {{A}} Comparative Review of Multiple Deep Neural Architectures},
  shorttitle = {Deep Learning for Electronic Health Records},
  author = {Ayala Solares, Jose Roberto and Diletta Raimondi, Francesca Elisa and Zhu, Yajie and Rahimian, Fatemeh and Canoy, Dexter and Tran, Jenny and Pinho Gomes, Ana Catarina and Payberah, Amir H. and Zottoli, Mariagrazia and Nazarzadeh, Milad and Conrad, Nathalie and Rahimi, Kazem and Salimi-Khorshidi, Gholamreza},
  date = {2020-01-01},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  volume = {101},
  pages = {103337},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2019.103337},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046419302564},
  urldate = {2024-05-20},
  abstract = {Despite the recent developments in deep learning models, their applications in clinical decision-support systems have been very limited. Recent digitalisation of health records, however, has provided a great platform for the assessment of the usability of such techniques in healthcare. As a result, the field is starting to see a growing number of research papers that employ deep learning on electronic health records (EHR) for personalised prediction of risks and health trajectories. While this can be a promising trend, vast paper-to-paper variability (from data sources and models they use to the clinical questions they attempt to answer) have hampered the field’s ability to simply compare and contrast such models for a given application of interest. Thus, in this paper, we aim to provide a comparative review of the key deep learning architectures that have been applied to EHR data. Furthermore, we also aim to: (1) introduce and use one of the world’s largest and most complex linked primary care EHR datasets (i.e., Clinical Practice Research Datalink, or CPRD) as a new asset for training such data-hungry models; (2) provide a guideline for working with EHR data for deep learning; (3) share some of the best practices for assessing the “goodness” of deep-learning models in clinical risk prediction; (4) and propose future research ideas for making deep learning models more suitable for the EHR data. Our results highlight the difficulties of working with highly imbalanced datasets, and show that sequential deep learning architectures such as RNN may be more suitable to deal with the temporal nature of EHR.},
  keywords = {CPRD,Deep learning,Electronic health records,Neural networks,notion,Representation learning},
  annotation = {Priority: 1},
  file = {/Users/mikhail/Zotero/storage/7UTMZE8S/Ayala Solares et al. - 2020 - Deep learning for electronic health records A com.pdf;/Users/mikhail/Zotero/storage/93YSEEKI/S1532046419302564.html}
}

@online{DeepTransformerModels2020,
  title = {Deep {{Transformer Models}} for {{Time Series Forecasting}}: {{The Influenza Prevalence Case}}},
  shorttitle = {Deep {{Transformer Models}} for {{Time Series Forecasting}}},
  author = {Wu, Neo and Green, Bradley and Ben, Xue and O'Banion, Shawn},
  date = {2020},
  doi = {10.48550/ARXIV.2001.08317},
  url = {https://arxiv.org/abs/2001.08317},
  urldate = {2024-05-19},
  abstract = {In this paper, we present a new approach to time series forecasting. Time series data are prevalent in many scientific and engineering disciplines. Time series forecasting is a crucial task in modeling time series data, and is an important area of machine learning. In this work we developed a novel method that employs Transformer-based machine learning models to forecast time series data. This approach works by leveraging self-attention mechanisms to learn complex patterns and dynamics from time series data. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings. Using influenza-like illness (ILI) forecasting as a case study, we show that the forecasting results produced by our approach are favorably comparable to the state-of-the-art.},
  pubstate = {prepublished},
  version = {1},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),notion},
  file = {/Users/mikhail/Zotero/storage/I7J3U4X9/Wu et al. - 2020 - Deep Transformer Models for Time Series Forecastin.pdf}
}

@inproceedings{DescEmb2022,
  title = {Unifying {{Heterogeneous Electronic Health Records Systems}} via {{Text-Based Code Embedding}}},
  booktitle = {Proceedings of the {{Conference}} on {{Health}}, {{Inference}}, and {{Learning}}},
  author = {Hur, Kyunghoon and Lee, Jiyoung and Oh, Jungwoo and Price, Wesley and Kim, Younghak and Choi, Edward},
  date = {2022-04-06},
  pages = {183--203},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v174/hur22a.html},
  urldate = {2024-07-02},
  abstract = {Increase in the use of Electronic Health Records (EHRs) has facilitated advances in predictive healthcare. However, EHR systems lack a unified code system for representing medical concepts. Heterogeneous formats of EHR present a barrier for the training and deployment of state-of-the-art deep learning models at scale. To overcome this problem, we introduce Description-based Embedding, DescEmb, a code-agnostic description-based representation learning framework for predictive modeling on EHR. DescEmb takes advantage of the flexibility of neural language models while maintaining a neutral approach that can be combined with prior frameworks for task-specific representation learning or predictive modeling. We test our model’s capacity on various experiments including prediction tasks, transfer learning and pooled learning. DescEmb shows higher performance in overall experiments compared to the code-based approach, opening the door to a text-based approach in predictive healthcare research that is not constrained by EHR structure nor special domain knowledge.},
  eventtitle = {Conference on {{Health}}, {{Inference}}, and {{Learning}}},
  langid = {english},
  keywords = {notion},
  annotation = {Github: https://github.com/hoon9405/DescEmb/tree/master},
  file = {/Users/mikhail/Zotero/storage/YUCNB7XI/Hur et al. - 2022 - Unifying Heterogeneous Electronic Health Records S.pdf}
}

@article{DevelopmentTransformerbasedNeural,
  title = {Development of a {{Transformer-based}} Neural Network for {{Biomarkers}} Prediction},
  author = {Shavliuk, Mikhail},
  abstract = {The abstract is a concise, self-containing one page description of the work: what was the problem, what was done, and what are the results. Do not include charts or tables in the abstract. Put the abstract in the primary language of your thesis first and then the translation when that is needed. International students do not need to include an abstract in Finnish.},
  langid = {english},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/DD3J9LWL/Shavliuk - Development of a Transformer-based neural network .pdf}
}

@inproceedings{DifferentialEvolutionAlgorithm2023,
  title = {Differential {{Evolution Algorithm}} Based {{Hyper-Parameters Selection}} of {{Transformer Neural Network Model}} for {{Load Forecasting}}},
  booktitle = {2023 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Sen, Anuvab and Rhik Mazumder, Arul and Sen, Udayon},
  date = {2023-12},
  pages = {234--239},
  issn = {2472-8322},
  doi = {10.1109/SSCI52147.2023.10371846},
  url = {https://ieeexplore.ieee.org/document/10371846},
  urldate = {2024-05-22},
  abstract = {Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in load forecasting. Transformer models have the potential to improve load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer-based Neural Network model integrated with different metaheuristic algorithms by their performance in load forecasting based on numerical metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE). Our findings demonstrate the potential of metaheuristic-enhanced Transformer-based Neural Network models in load forecasting accuracy and provide optimal hyperparameters for each model.},
  eventtitle = {2023 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  keywords = {Computational modeling,Deep Learning,Differential Evolution,Genetic Algorithm,Load forecasting,Measurement,Meta-heuristics,Metaheuristics,Neural networks,notion,Particle Swarm Optimization,Power system dynamics,Predictive models},
  file = {/Users/mikhail/Zotero/storage/WC252GZF/Sen et al. - 2023 - Differential Evolution Algorithm based Hyper-Param.pdf;/Users/mikhail/Zotero/storage/XC2E3JEJ/10371846.html}
}

@incollection{DigitalImageProcessing,
  title = {Digital Image Processing},
  booktitle = {Digital Image Processing},
  author = {Gonzalez, Rafael C. and Woods, Richard E. (Richard Eugene)},
  date = {2018/2018},
  edition = {Fourth edition, Global edition.},
  publisher = {Pearson Education Limited},
  location = {Harlow, Essex, England},
  isbn = {978-1-292-22304-9},
  langid = {english},
  keywords = {Image processing -- Digital techniques}
}

@article{DropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  urldate = {2024-10-03},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {/Users/mikhail/Zotero/storage/8KVHNY3C/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf}
}

@online{EHRFL2024,
  title = {{{EHRFL}}: {{Federated Learning Framework}} for {{Heterogeneous EHRs}} and {{Precision-guided Selection}} of {{Participating Clients}}},
  shorttitle = {{{EHRFL}}},
  author = {Kim, Jiyoun and Kim, Junu and Hur, Kyunghoon and Choi, Edward},
  date = {2024-04-20},
  eprint = {2404.13318},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.13318},
  url = {http://arxiv.org/abs/2404.13318},
  urldate = {2024-05-21},
  abstract = {In this study, we provide solutions to two practical yet overlooked scenarios in federated learning for electronic health records (EHRs): firstly, we introduce EHRFL, a framework that facilitates federated learning across healthcare institutions with distinct medical coding systems and database schemas using text-based linearization of EHRs. Secondly, we focus on a scenario where a single healthcare institution initiates federated learning to build a model tailored for itself, in which the number of clients must be optimized in order to reduce expenses incurred by the host. For selecting participating clients, we present a novel precision-based method, leveraging data latents to identify suitable participants for the institution. Our empirical results show that EHRFL effectively enables federated learning across hospitals with different EHR systems. Furthermore, our results demonstrate the efficacy of our precision-based method in selecting reduced number of participating clients without compromising model performance, resulting in lower operational costs when constructing institution-specific models. We believe this work lays a foundation for the broader adoption of federated learning on EHRs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/Users/mikhail/Zotero/storage/NSAQYU9K/Kim et al. - 2024 - EHRFL Federated Learning Framework for Heterogene.pdf;/Users/mikhail/Zotero/storage/RI7IP29F/2404.html}
}

@inproceedings{EHRTimeseriesBenchmark2021,
  title = {A Comprehensive {{EHR}} Timeseries Pre-Training Benchmark},
  booktitle = {Proceedings of the {{Conference}} on {{Health}}, {{Inference}}, and {{Learning}}},
  author = {McDermott, Matthew and Nestor, Bret and Kim, Evan and Zhang, Wancong and Goldenberg, Anna and Szolovits, Peter and Ghassemi, Marzyeh},
  date = {2021-04-08},
  pages = {257--278},
  publisher = {ACM},
  location = {Virtual Event USA},
  doi = {10.1145/3450439.3451877},
  url = {https://dl.acm.org/doi/10.1145/3450439.3451877},
  urldate = {2024-06-19},
  eventtitle = {{{ACM CHIL}} '21: {{ACM Conference}} on {{Health}}, {{Inference}}, and {{Learning}}},
  isbn = {978-1-4503-8359-2},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 1},
  file = {/Users/mikhail/Zotero/storage/TWNBVWJJ/McDermott et al. - 2021 - A comprehensive EHR timeseries pre-training benchm.pdf}
}

@software{elicit,
  title = {Elicit: {{The AI}} Research Assistant},
  author = {{Elicit}},
  year = {2023-01-24, 2023},
  url = {https://elicit.com}
}

@article{emmert2016need,
  title = {The Need for Formally Defining “{{Modern Medicine}}” by Means of Experimental Design},
  author = {Emmert-Streib, Frank and Tuomisto, Lauri and Yli-Harja, Olli},
  date = {2016},
  journaltitle = {Frontiers in Genetics},
  volume = {7},
  pages = {60},
  publisher = {Frontiers Media SA}
}

@article{emmert2018machine,
  title = {A Machine Learning Perspective on {{Personalized Medicine}}: An Automized, Comprehensive Knowledge Base with Ontology for Pattern Recognition},
  author = {Emmert-Streib, Frank and Dehmer, Matthias},
  date = {2018},
  journaltitle = {Machine Learning and Knowledge Extraction},
  volume = {1},
  number = {1},
  pages = {149--156},
  publisher = {MDPI}
}

@article{emmert2019comprehensive,
  title = {A Comprehensive Survey of Error Measures for Evaluating Binary Decision Making in Data Science},
  author = {Emmert-Streib, Frank and Moutari, Salisou and Dehmer, Matthias},
  date = {2019},
  journaltitle = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume = {9},
  number = {5},
  pages = {e1303},
  publisher = {Wiley Online Library}
}

@article{emmert2019evaluation,
  title = {Evaluation of Regression Models: {{Model}} Assessment, Model Selection and Generalization Error},
  author = {Emmert-Streib, Frank and Dehmer, Matthias},
  date = {2019},
  journaltitle = {Machine learning and knowledge extraction},
  volume = {1},
  number = {1},
  pages = {521--551},
  publisher = {MDPI}
}

@article{emmert2019introduction,
  title = {Introduction to Survival Analysis in Practice},
  author = {Emmert-Streib, Frank and Dehmer, Matthias},
  date = {2019},
  journaltitle = {Machine Learning and Knowledge Extraction},
  volume = {1},
  number = {3},
  pages = {1013--1038},
  publisher = {MDPI}
}

@article{emmert2020artificial,
  title = {Artificial Intelligence: {{A}} Clarification of Misconceptions, Myths and Desired Status},
  author = {Emmert-Streib, Frank and Yli-Harja, Olli and Dehmer, Matthias},
  date = {2020},
  journaltitle = {Frontiers in artificial intelligence},
  volume = {3},
  pages = {524339},
  publisher = {Frontiers Media SA}
}

@article{emmert2020introductory,
  title = {An Introductory Review of Deep Learning for Prediction Models with Big Data},
  author = {Emmert-Streib, Frank and Yang, Zhen and Feng, Han and Tripathi, Shailesh and Dehmer, Matthias},
  date = {2020},
  journaltitle = {Frontiers in Artificial Intelligence},
  volume = {3},
  pages = {4},
  publisher = {Frontiers Media SA}
}

@book{emmert2022mathematical,
  title = {Mathematical Foundations of Data Science Using {{R}}},
  author = {Emmert-Streib, Frank and Moutari, Salissou and Dehmer, Matthias},
  date = {2022},
  publisher = {Walter de Gruyter GmbH \& Co KG}
}

@book{emmert2023elements,
  title = {Elements of Data Science, Machine Learning, and Artificial Intelligence Using {{R}}},
  author = {Emmert-Streib, Frank and Moutari, Salissou and Dehmer, Matthias},
  publisher = {Springer}
}

@article{ExplainableAnticancerCompound2019,
  title = {Toward {{Explainable Anticancer Compound Sensitivity Prediction}} via {{Multimodal Attention-Based Convolutional Encoders}}},
  author = {Manica, Matteo and Oskooei, Ali and Born, Jannis and Subramanian, Vigneshwari and Sáez-Rodríguez, Julio and Rodríguez Martínez, María},
  date = {2019-12-02},
  journaltitle = {Molecular Pharmaceutics},
  shortjournal = {Mol. Pharmaceutics},
  volume = {16},
  number = {12},
  pages = {4797--4806},
  publisher = {American Chemical Society},
  issn = {1543-8384},
  doi = {10.1021/acs.molpharmaceut.9b00520},
  url = {https://doi.org/10.1021/acs.molpharmaceut.9b00520},
  urldate = {2024-05-20},
  abstract = {In line with recent advances in neural drug design and sensitivity prediction, we propose a novel architecture for interpretable prediction of anticancer compound sensitivity using a multimodal attention-based convolutional encoder. Our model is based on the three key pillars of drug sensitivity: compounds’ structure in the form of a SMILES sequence, gene expression profiles of tumors, and prior knowledge on intracellular interactions from protein–protein interaction networks. We demonstrate that our multiscale convolutional attention-based encoder significantly outperforms a baseline model trained on Morgan fingerprints and a selection of encoders based on SMILES, as well as the previously reported state-of-the-art for multimodal drug sensitivity prediction (R2 = 0.86 and RMSE = 0.89). Moreover, the explainability of our approach is demonstrated by a thorough analysis of the attention weights. We show that the attended genes significantly enrich apoptotic processes and that the drug attention is strongly correlated with a standard chemical structure similarity index. Finally, we report a case study of two receptor tyrosine kinase (RTK) inhibitors acting on a leukemia cell line, showcasing the ability of the model to focus on informative genes and submolecular regions of the two compounds. The demonstrated generalizability and the interpretability of our model testify to its potential for in silico prediction of anticancer compound efficacy on unseen cancer cells, positioning it as a valid solution for the development of personalized therapies as well as for the evaluation of candidate compounds in de novo drug design.},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/QDVVCWAL/Manica et al. - 2019 - Toward Explainable Anticancer Compound Sensitivity.pdf}
}

@inproceedings{ExploringEffectNormalization2021,
  title = {Exploring the Effect of Normalization on Medical Data Classification},
  booktitle = {2021 {{International Conference}} on {{Artificial Intelligence}} and {{Machine Vision}} ({{AIMV}})},
  author = {Singh, Namrata and Singh, Pradeep},
  date = {2021-09},
  pages = {1--5},
  doi = {10.1109/AIMV53313.2021.9670938},
  url = {https://ieeexplore.ieee.org/document/9670938},
  urldate = {2024-11-06},
  abstract = {Data normalization as one of the pre-processing strategies is utilized either to transform or scale the data in order to make an equal contribution of each attribute. For a given classification problem, the performance of any machine learning approach depends upon the quality of data in order to produce a generalized classification approach. Various studies have shown the significance of data normalization to enhance the quality of data and finally the performance of machine learning techniques. But there is dearth of investigations about the effect of data normalization methods in classifying the medical datasets. Thus, this study intends to explore the effect of three data normalization techniques namely min-max, z-score and Median and Median Absolute Deviation on the performance of four classification algorithms namely Naïve Bayes, Support Vector Machine - Radial Basis Function, Random Forest and k-Nearest Neighbour. The experiments conducted on 20 publicly available medical datasets are based on the classification accuracy as performance parameter. The best performance results were obtained with z-score normalization method along with Random Forest classifier.},
  eventtitle = {2021 {{International Conference}} on {{Artificial Intelligence}} and {{Machine Vision}} ({{AIMV}})},
  keywords = {Classification,Classification algorithms,Data normalization,Forestry,k-Nearest Neighbour,Machine vision,Measurement,Naïve Bayes,Predictive models,Random Forest,Support vector machines,Support Vector Machines,Transforms},
  file = {/Users/mikhail/Zotero/storage/VTS6GT33/Singh and Singh - 2021 - Exploring the effect of normalization on medical data classification.pdf;/Users/mikhail/Zotero/storage/5IKMFJWH/9670938.html}
}

@inproceedings{ExploringEffectNormalization2021a,
  title = {Exploring the Effect of Normalization on Medical Data Classification},
  booktitle = {2021 {{International Conference}} on {{Artificial Intelligence}} and {{Machine Vision}} ({{AIMV}})},
  author = {Singh, Namrata and Singh, Pradeep},
  date = {2021-09},
  pages = {1--5},
  doi = {10.1109/AIMV53313.2021.9670938},
  url = {https://ieeexplore.ieee.org/document/9670938},
  urldate = {2024-11-06},
  abstract = {Data normalization as one of the pre-processing strategies is utilized either to transform or scale the data in order to make an equal contribution of each attribute. For a given classification problem, the performance of any machine learning approach depends upon the quality of data in order to produce a generalized classification approach. Various studies have shown the significance of data normalization to enhance the quality of data and finally the performance of machine learning techniques. But there is dearth of investigations about the effect of data normalization methods in classifying the medical datasets. Thus, this study intends to explore the effect of three data normalization techniques namely min-max, z-score and Median and Median Absolute Deviation on the performance of four classification algorithms namely Naïve Bayes, Support Vector Machine - Radial Basis Function, Random Forest and k-Nearest Neighbour. The experiments conducted on 20 publicly available medical datasets are based on the classification accuracy as performance parameter. The best performance results were obtained with z-score normalization method along with Random Forest classifier.},
  eventtitle = {2021 {{International Conference}} on {{Artificial Intelligence}} and {{Machine Vision}} ({{AIMV}})},
  keywords = {Classification,Classification algorithms,Data normalization,Forestry,k-Nearest Neighbour,Machine vision,Measurement,Naïve Bayes,Predictive models,Random Forest,Support vector machines,Support Vector Machines,Transforms}
}

@article{ExtensiveDataProcessing2022,
  title = {An {{Extensive Data Processing Pipeline}} for {{MIMIC-IV}}},
  author = {Gupta, Mehak and Gallamoza, Brennan and Cutrona, Nicolas and Dhakal, Pranjal and Poulain, Raphael and Beheshti, Rahmatollah},
  date = {2022-11},
  journaltitle = {Proceedings of machine learning research},
  shortjournal = {Proc Mach Learn Res},
  volume = {193},
  eprint = {36686986},
  eprinttype = {pmid},
  pages = {311--325},
  issn = {2640-3498},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9854277/},
  urldate = {2024-07-04},
  abstract = {An increasing amount of research is being devoted to applying machine learning methods to electronic health record (EHR) data for various clinical purposes. This growing area of research has exposed the challenges of the accessibility of EHRs. MIMIC is a popular, public, and free EHR dataset in a raw format that has been used in numerous studies. The absence of standardized preprocessing steps can be, however, a significant barrier to the wider adoption of this rare resource. Additionally, this absence can reduce the reproducibility of the developed tools and limit the ability to compare the results among similar studies. In this work, we provide a greatly customizable pipeline to extract, clean, and preprocess the data available in the fourth version of the MIMIC dataset (MIMIC-IV). The pipeline also presents an end-to-end wizard-like package supporting predictive model creations and evaluations. The pipeline covers a range of clinical prediction tasks which can be broadly classified into four categories - readmission, length of stay, mortality, and phenotype prediction. The tool is publicly available at https://github.com/healthylaife/MIMIC-IV-Data-Pipeline.},
  pmcid = {PMC9854277},
  file = {/Users/mikhail/Zotero/storage/JUDP4L7W/Gupta et al. - 2022 - An Extensive Data Processing Pipeline for MIMIC-IV.pdf}
}

@online{gelu,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2023-06-05},
  eprint = {1606.08415},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.08415},
  url = {http://arxiv.org/abs/1606.08415},
  urldate = {2024-08-20},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x\textbackslash Phi(x)\$, where \$\textbackslash Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x\textbackslash mathbf\{1\}\_\{x{$>$}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  pubstate = {prepublished},
  version = {5},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/mikhail/Zotero/storage/FCSLEPDK/Hendrycks and Gimpel - 2023 - Gaussian Error Linear Units (GELUs).pdf;/Users/mikhail/Zotero/storage/PBUZ5J5R/1606.html}
}

@inproceedings{GeneralFinetunedTransfer2019,
  title = {A {{General Fine-tuned Transfer Learning Model}} for {{Predicting Clinical Task Acrossing Diverse EHRs Datasets}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Sun, Zhe and Peng, Shaoliang and Yang, Yaning and Wang, Xiaoqi and Li, Fei},
  date = {2019-11},
  pages = {490--495},
  publisher = {IEEE},
  location = {San Diego, CA, USA},
  doi = {10.1109/BIBM47256.2019.8983098},
  url = {https://ieeexplore.ieee.org/document/8983098/},
  urldate = {2024-06-19},
  abstract = {Data analysis of electronic health record (EHRs) system using machine learning, statistical methods can predict relevant clinical tasks. However, there is no uniform standard for current electronic health record systems, and the clinical outcome prediction models trained on one EHR dataset cannot be applied well on other EHR datasets from different medical institutions. Data differences between different medical institutions pose a huge challenge to the study of electronic health records. In this study, we proposed a general transfer learning strategy which can enable models to make clinical prediction acrossing diverse EHRs datasets and validated its strong versatility on three deep learning models. Two different intensive care units (ICU) databases (MIMIC-III and eICU) and one clinical task (in-hospital mortality) are used to evaluate our method. At first, we trained the deep learning models on the source dataset and saved the model states after each epoch. Then, we selected the best performing model as the pre-training model, transferred it to the target dataset and fine-tuned the whole network on target dataset. Finally, we use the fine-tuned models to make predictions on the target dataset. Experiment results show that AUROC score increased by 3\%-20\% with transfer strategy, which indicated that the general strategy can provide more reliable predictions acrossing EHRs databases to predict clinical tasks.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  isbn = {978-1-72811-867-3},
  langid = {english},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/RM6ZKWN8/Sun et al. - 2019 - A General Fine-tuned Transfer Learning Model for P.pdf}
}

@inproceedings{GenerativeAdversarialNetwork2020,
  title = {Generative {{Adversarial Network}} for {{Robust Regression}} Using {{Continuous Dataset}}},
  booktitle = {2020 {{International Conference}} on {{Information}} and {{Communication Technology Convergence}} ({{ICTC}})},
  author = {Min, Yu-Lim and Hong, Seung-Jin and Kim, Hye-jin and Lee, Seung-Ik},
  date = {2020-10},
  pages = {1209--1211},
  issn = {2162-1233},
  doi = {10.1109/ICTC49870.2020.9289188},
  url = {https://ieeexplore.ieee.org/document/9289188},
  urldate = {2024-05-22},
  abstract = {Recently, advanced neural network, which is implementing technical method, has focus on dealing with image classification problems. Unlike classification problems, regression provides a value of output in complex and sophisticated continuous datasets. Though nonlinear models can perform regression better than linear model as Linear Regression(LR), the difficulty to make robust model still remain. In this paper, our purpose is to design training architecture for robust regression. We approach Neural Network known as nonlinear regression to solve limitation of Linear Regression. Additionally, Our architecture uses a new artificial Neural Network(NN) based on adversarial architecture by using the Generator(G) and Discriminator(D). The Discriminator shows the better performance while competing with the Generator and learning regression problem as same time. In evaluation experiments, we compare our proposed model with baseline models including Linear Regression and Neural Network using continuous real world data. We split four datasets into train and test sets as 90:10 and evaluate them by using Mean Squared Error(MSE) function. In summary, our model trained with Generative Adversarial Network(GAN) shows better performance than the baseline models.},
  eventtitle = {2020 {{International Conference}} on {{Information}} and {{Communication Technology Convergence}} ({{ICTC}})},
  keywords = {Adversarial Architecture,Artificial neural networks,Data models,Discriminator,Generator,Generators,Image classification,Information and communication technology,Linear regression,Linear Regression,Neural Network,notion,Training},
  file = {/Users/mikhail/Zotero/storage/VXPWDDJF/Min et al. - 2020 - Generative Adversarial Network for Robust Regressi.pdf;/Users/mikhail/Zotero/storage/92XT9RSW/9289188.html}
}

@article{GenHPF2024,
  title = {{{GenHPF}}: {{General Healthcare Predictive Framework}} for {{Multi-Task Multi-Source Learning}}},
  shorttitle = {{{GenHPF}}},
  author = {Hur, Kyunghoon and Oh, Jungwoo and Kim, Junu and Kim, Jiyoun and Lee, Min Jae and Cho, Eunbyeol and Moon, Seong-Eun and Kim, Young-Hak and Atallah, Louis and Choi, Edward},
  date = {2024-01},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  volume = {28},
  number = {1},
  pages = {502--513},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2023.3327951},
  url = {https://ieeexplore.ieee.org/document/10298642},
  urldate = {2024-05-21},
  abstract = {Despite the remarkable progress in the development of predictive models for healthcare, applying these algorithms on a large scale has been challenging. Algorithms trained on a particular task, based on specific data formats available in a set of medical records, tend to not generalize well to other tasks or databases in which the data fields may differ. To address this challenge, we propose General Healthcare Predictive Framework (GenHPF), which is applicable to any EHR with minimal preprocessing for multiple prediction tasks. GenHPF resolves heterogeneity in medical codes and schemas by converting EHRs into a hierarchical textual representation while incorporating as many features as possible. To evaluate the efficacy of GenHPF, we conduct multi-task learning experiments with single-source and multi-source settings, on three publicly available EHR datasets with different schemas for 12 clinically meaningful prediction tasks. Our framework significantly outperforms baseline models that utilize domain knowledge in multi-source learning, improving average AUROC by 1.2\%P in pooled learning and 2.6\%P in transfer learning while also showing comparable results when trained on a single EHR dataset. Furthermore, we demonstrate that self-supervised pretraining using multi-source datasets is effective when combined with GenHPF, resulting in a 0.6\%P AUROC improvement compared to models without pretraining. By eliminating the need for preprocessing and feature engineering, we believe that this work offers a solid framework for multi-task and multi-source learning that can be leveraged to speed up the scaling and usage of predictive algorithms in healthcare.},
  eventtitle = {{{IEEE Journal}} of {{Biomedical}} and {{Health Informatics}}},
  keywords = {Codes,Data models,Electronic health records,heterogeneity,Hospitals,Medical diagnostic imaging,multi-source learning,multi-task learning,Multitasking,natural language process,notion,Predictive models,Task analysis},
  file = {/Users/mikhail/Zotero/storage/PUY2P6FM/Hur et al. - 2024 - GenHPF General Healthcare Predictive Framework fo.pdf;/Users/mikhail/Zotero/storage/HHEZ4BBM/10298642.html}
}

@article{GlucoseTransformerForecasting2023,
  title = {Glucose {{Transformer}}: {{Forecasting Glucose Level}} and {{Events}} of {{Hyperglycemia}} and {{Hypoglycemia}}},
  shorttitle = {Glucose {{Transformer}}},
  author = {Lee, Sang-Min and Kim, Dae-Yeon and Woo, Jiyoung},
  date = {2023-03},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  shortjournal = {IEEE J. Biomed. Health Inform.},
  volume = {27},
  number = {3},
  pages = {1600--1611},
  issn = {2168-2194, 2168-2208},
  doi = {10.1109/JBHI.2023.3236822},
  url = {https://ieeexplore.ieee.org/document/10035395/},
  urldate = {2024-05-19},
  keywords = {notion},
  annotation = {Priority: 1},
  file = {/Users/mikhail/Zotero/storage/VDWUADGR/Lee et al. - 2023 - Glucose Transformer Forecasting Glucose Level and.pdf}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press}
}

@online{GraphGuidedNetworkIrregularly2022,
  title = {Graph-{{Guided Network}} for {{Irregularly Sampled Multivariate Time Series}}},
  author = {Zhang, Xiang and Zeman, Marko and Tsiligkaridis, Theodoros and Zitnik, Marinka},
  date = {2022-03-15},
  eprint = {2110.05357},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.05357},
  url = {http://arxiv.org/abs/2110.05357},
  urldate = {2024-07-11},
  abstract = {In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with varying time intervals between successive readouts and different subsets of variables (sensors) observed at different time points. Here, we introduce RAINDROP, a graph neural network that embeds irregularly sampled and multivariate time series while also learning the dynamics of sensors purely from observational data. RAINDROP represents every sample as a separate sensor graph and models time-varying dependencies between sensors with a novel message passing operator. It estimates the latent sensor graph structure and leverages the structure together with nearby observations to predict misaligned readouts. This model can be interpreted as a graph neural network that sends messages over graphs that are optimized for capturing time-varying dependencies among sensors. We use RAINDROP to classify time series and interpret temporal dynamics on three healthcare and human activity datasets. RAINDROP outperforms state-of-the-art methods by up to 11.4\% (absolute F1-score points), including techniques that deal with irregular sampling using fixed discretization and set functions. RAINDROP shows superiority in diverse setups, including challenging leave-sensor-out settings.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/mikhail/Zotero/storage/R3NILV2H/Zhang et al. - 2022 - Graph-Guided Network for Irregularly Sampled Multi.pdf;/Users/mikhail/Zotero/storage/Y47BP6UN/2110.html}
}

@online{gru,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and family=Merrienboer, given=Bart, prefix=van, useprefix=true and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  date = {2014-09-02},
  eprint = {1406.1078},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1406.1078},
  url = {http://arxiv.org/abs/1406.1078},
  urldate = {2024-09-20},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/mikhail/Zotero/storage/IFINLBCX/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf;/Users/mikhail/Zotero/storage/U4NZXAKW/1406.html}
}

@online{gru-evaluation,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  date = {2014-12-11},
  eprint = {1412.3555},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.3555},
  url = {http://arxiv.org/abs/1412.3555},
  urldate = {2024-08-26},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/mikhail/Zotero/storage/929BB4L6/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf;/Users/mikhail/Zotero/storage/MT7E493E/1412.html}
}

@article{GRUD2018,
  title = {Recurrent {{Neural Networks}} for {{Multivariate Time Series}} with {{Missing Values}}},
  author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
  date = {2018-04-17},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {8},
  number = {1},
  pages = {6085},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-24271-9},
  url = {https://www.nature.com/articles/s41598-018-24271-9},
  urldate = {2024-08-26},
  abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.},
  langid = {english},
  keywords = {Computational models,Computer science,Machine learning},
  file = {/Users/mikhail/Zotero/storage/9KNAQP35/Che et al. - 2018 - Recurrent Neural Networks for Multivariate Time Se.pdf}
}

@article{HiBEHRT2023,
  title = {Hi-{{BEHRT}}: {{Hierarchical Transformer-Based Model}} for {{Accurate Prediction}} of {{Clinical Events Using Multimodal Longitudinal Electronic Health Records}}},
  shorttitle = {Hi-{{BEHRT}}},
  author = {Li, Yikuan and Mamouei, Mohammad and Salimi-Khorshidi, Gholamreza and Rao, Shishir and Hassaine, Abdelaali and Canoy, Dexter and Lukasiewicz, Thomas and Rahimi, Kazem},
  date = {2023-02},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  shortjournal = {IEEE J. Biomed. Health Inform.},
  volume = {27},
  number = {2},
  pages = {1106--1117},
  issn = {2168-2194, 2168-2208},
  doi = {10.1109/JBHI.2022.3224727},
  url = {https://ieeexplore.ieee.org/document/9964038/},
  urldate = {2024-05-19},
  keywords = {notion},
  annotation = {Priority: 3},
  file = {/Users/mikhail/Zotero/storage/C4659CRB/HiBEHRT.pdf}
}

@inproceedings{HierarchicalTransformers2019,
  title = {Hierarchical {{Transformers}} for {{Long Document Classification}}},
  booktitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Pappagari, Raghavendra and Zelasko, Piotr and Villalba, Jesus and Carmiel, Yishay and Dehak, Najim},
  date = {2019-12},
  pages = {838--844},
  publisher = {IEEE},
  location = {SG, Singapore},
  doi = {10.1109/ASRU46091.2019.9003958},
  url = {https://ieeexplore.ieee.org/document/9003958/},
  urldate = {2024-06-24},
  eventtitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  isbn = {978-1-72810-306-8},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/P6BJXD98/Pappagari et al. - 2019 - Hierarchical Transformers for Long Document Classi.pdf}
}

@article{HiTANet2020,
  title = {{{HiTANet}}: {{Hierarchical Time-Aware Attention Networks}} for {{Risk Prediction}} on {{Electronic Health Records}}},
  shorttitle = {{{HiTANet}}},
  author = {Luo, Junyu and Ye, Muchao and Xiao, Cao and Ma, Fenglong},
  date = {2020-08-23},
  journaltitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages = {647--656},
  doi = {10.1145/3394486.3403107},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403107},
  urldate = {2024-05-18},
  abstract = {Deep learning methods especially recurrent neural network based models have demonstrated early success in disease risk prediction on longitudinal patient data. Existing works follow a strong assumption to implicitly assume the stationary disease progression during each time period, and thus, take a homogeneous way to decay the information from previous time steps for all patients. However,in reality, disease progression is non-stationary. Besides, the key time steps for a target disease vary among patients. To leverage time information for risk prediction in a more reasonable way, we propose a new hierarchical time-aware attention network, named HiTANet, which imitates the decision making process of doctors inrisk prediction. Particularly, HiTANet models time information in local and global stages. The local evaluation stage has a time aware Transformer that embeds time information into visit-level embed-ding and generates local attention weight for each visit. The global synthesis stage further adopts a time-aware key-query attention mechanism to assign global weights to different time steps. Finally, the two types of attention weights are dynamically combined to generate the patient representations for further risk prediction. We evaluate HiTANet on three real-world datasets. Compared with the best results among twelve competing baselines, HiTANet achieves over 7\% in terms of F1 score on all datasets, which demonstrates the effectiveness of the proposed model and the necessity of modeling time information in risk prediction task.},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 2},
  file = {/Users/mikhail/Zotero/storage/EXGLFGBM/HiTANet.pdf}
}

@article{HuberLoss1964,
  title = {Robust {{Estimation}} of a {{Location Parameter}}},
  author = {Huber, Peter J.},
  date = {1964-03},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {35},
  number = {1},
  pages = {73--101},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177703732},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full},
  urldate = {2024-10-01},
  abstract = {This paper contains a new approach toward a theory of robust estimation; it treats in detail the asymptotic theory of estimating a location parameter for contaminated normal distributions, and exhibits estimators--intermediaries between sample mean and sample median--that are asymptotically most robust (in a sense to be specified) among all translation invariant estimators. For the general background, see Tukey (1960) (p. 448 ff.) Let \$x\_1, \textbackslash cdots, x\_n\$ be independent random variables with common distribution function \$F(t - \textbackslash xi)\$. The problem is to estimate the location parameter \$\textbackslash xi\$, but with the complication that the prototype distribution \$F(t)\$ is only approximately known. I shall primarily be concerned with the model of indeterminacy \$F = (1 - \textbackslash epsilon)\textbackslash Phi + \textbackslash epsilon H\$, where \$0 \textbackslash leqq \textbackslash epsilon {$<$} 1\$ is a known number, \$\textbackslash Phi(t) = (2\textbackslash pi)\textasciicircum\{-\textbackslash frac\{1\}\{2\}\} \textbackslash int\textasciicircum t\_\{-\textbackslash infty\} \textbackslash exp(-\textbackslash frac\{1\}\{2\}s\textasciicircum 2) ds\$ is the standard normal cumulative and \$H\$ is an unknown contaminating distribution. This model arises for instance if the observations are assumed to be normal with variance 1, but a fraction \$\textbackslash epsilon\$ of them is affected by gross errors. Later on, I shall also consider other models of indeterminacy, e.g., \$\textbackslash sup\_t |F(t) - \textbackslash Phi(t)| \textbackslash leqq \textbackslash epsilon\$. Some inconvenience is caused by the fact that location and scale parameters are not uniquely determined: in general, for fixed \$\textbackslash epsilon\$, there will be several values of \$\textbackslash xi\$ and \$\textbackslash sigma\$ such that \$\textbackslash sup\_t|F(t) - \textbackslash Phi((t - \textbackslash xi)/\textbackslash sigma)| \textbackslash leqq \textbackslash epsilon\$, and similarly for the contaminated case. Although this inherent and unavoidable indeterminacy is small if \$\textbackslash epsilon\$ is small and is rather irrelevant for practical purposes, it poses awkward problems for the theory, especially for optimality questions. To remove this difficulty, one may either (i) restrict attention to symmetric distributions, and estimate the location of the center of symmetry (this works for \$\textbackslash xi\$ but not for \$\textbackslash sigma\$); or (ii) one may define the parameter to be estimated in terms of the estimator itself, namely by its asymptotic value for sample size \$n \textbackslash rightarrow \textbackslash infty\$; or (iii) one may define the parameters by arbitrarily chosen functionals of the distribution (e.g., by the expectation, or the median of \$F\$). All three possibilities have unsatisfactory aspects, and I shall usually choose the variant which is mathematically most convenient. It is interesting to look back to the very origin of the theory of estimation, namely to Gauss and his theory of least squares. Gauss was fully aware that his main reason for assuming an underlying normal distribution and a quadratic loss function was mathematical, i.e., computational, convenience. In later times, this was often forgotten, partly because of the central limit theorem. However, if one wants to be honest, the central limit theorem can at most explain why many distributions occurring in practice are approximately normal. The stress is on the word "approximately." This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): What happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance: seemingly quite mild deviations may already explode its variance. Tukey and others proposed several more robust substitutes--trimmed means, Winsorized means, etc.--and explored their performance for a few typical violations of normality. A general theory of robust estimation is still lacking; it is hoped that the present paper will furnish the first few steps toward such a theory. At the core of the method of least squares lies the idea to minimize the sum of the squared "errors," that is, to adjust the unknown parameters such that the sum of the squares of the differences between observed and computed values is minimized. In the simplest case, with which we are concerned here, namely the estimation of a location parameter, one has to minimize the expression \$\textbackslash sum\_i (x\_i - T)\textasciicircum 2\$; this is of course achieved by the sample mean \$T = \textbackslash sum\_i x\_i/n\$. I should like to emphasize that no loss function is involved here; I am only describing how the least squares estimator is defined, and neither the underlying family of distributions nor the true value of the parameter to be estimated enters so far. It is quite natural to ask whether one can obtain more robustness by minimizing another function of the errors than the sum of their squares. We shall therefore concentrate our attention to estimators that can be defined by a minimum principle of the form (for a location parameter): \$T = T\_n(x\_1, \textbackslash cdots, x\_n) minimizes \textbackslash sum\_i \textbackslash rho(x\_i - T),\$ \textbackslash begin\{equation*\} \textbackslash tag\{M\} where \textbackslash rho is a non-constant function. \textbackslash end\{equation*\} Of course, this definition generalizes at once to more general least squares type problems, where several parameters have to be determined. This class of estimators contains in particular (i) the sample mean \$(\textbackslash rho(t) = t\textasciicircum 2)\$, (ii) the sample median \$(\textbackslash rho(t) = |t|)\$, and more generally, (iii) all maximum likelihood estimators \$(\textbackslash rho(t) = -\textbackslash log f(t)\$, where \$f\$ is the assumed density of the untranslated distribution). These (\$M\$)-estimators, as I shall call them for short, have rather pleasant asymptotic properties; sufficient conditions for asymptotic normality and an explicit expression for their asymptotic variance will be given. How should one judge the robustness of an estimator \$T\_n(x) = T\_n(x\_1, \textbackslash cdots, x\_n)\$? Since ill effects from contamination are mainly felt for large sample sizes, it seems that one should primarily optimize large sample robustness properties. Therefore, a convenient measure of robustness for asymptotically normal estimators seems to be the supremum of the asymptotic variance \$(n \textbackslash rightarrow \textbackslash infty)\$ when \$F\$ ranges over some suitable set of underlying distributions, in particular over the set of all \$F = (1 - \textbackslash epsilon)\textbackslash Phi + \textbackslash epsilon H\$ for fixed \$\textbackslash epsilon\$ and symmetric \$H\$. On second thought, it turns out that the asymptotic variance is not only easier to handle, but that even for moderate values of \$n\$ it is a better measure of performance than the actual variance, because (i) the actual variance of an estimator depends very much on the behavior of the tails of \$H\$, and the supremum of the actual variance is infinite for any estimator whose value is always contained in the convex hull of the observations. (ii) If an estimator is asymptotically normal, then the important central part of its distribution and confidence intervals for moderate confidence levels can better be approximated in terms of the asymptotic variance than in terms of the actual variance. If we adopt this measure of robustness, and if we restrict attention to (\$M\$)-estimators, then it will be shown that the most robust estimator is uniquely determined and corresponds to the following \$\textbackslash rho:\textbackslash rho(t) = \textbackslash frac\{1\}\{2\}t\textasciicircum 2\$ for \$|t| {$<$} k, \textbackslash rho(t) = k|t| - \textbackslash frac\{1\}\{2\}k\textasciicircum 2\$ for \$|t| \textbackslash geqq k\$, with \$k\$ depending on \$\textbackslash epsilon\$. This estimator is most robust even among all translation invariant estimators. Sample mean \$(k = \textbackslash infty)\$ and sample median \$(k = 0)\$ are limiting cases corresponding to \$\textbackslash epsilon = 0\$ and \$\textbackslash epsilon = 1\$, respectively, and the estimator is closely related and asymptotically equivalent to Winsorizing. I recall the definition of Winsorizing: assume that the observations have been ordered, \$x\_1 \textbackslash leqq x\_2 \textbackslash leqq \textbackslash cdots \textbackslash leqq x\_n\$, then the statistic \$T = n\textasciicircum\{-1\}(gx\_\{g + 1\} + x\_\{g + 1\} + x\_\{g + 2\} + \textbackslash cdots + x\_\{n - h\} + hx\_\{n - h\})\$ is called the Winsorized mean, obtained by Winsorizing the \$g\$ leftmost and the \$h\$ rightmost observations. The above most robust (\$M\$)-estimators can be described by the same formula, except that in the first and in the last summand, the factors \$x\_\{g + 1\}\$ and \$x\_\{n - h\}\$ have to be replaced by some numbers \$u, v\$ satisfying \$x\_g \textbackslash leqq u \textbackslash leqq x\_\{g + 1\}\$ and \$x\_\{n - h\} \textbackslash leqq v \textbackslash leqq x\_\{n - h + 1\}\$, respectively; \$g, h, u\$ and \$v\$ depend on the sample. In fact, this (\$M\$)-estimator is the maximum likelihood estimator corresponding to a unique least favorable distribution \$F\_0\$ with density \$f\_0(t) = (1 - \textbackslash epsilon)(2\textbackslash pi)\textasciicircum\{-\textbackslash frac\{1\}\{2\}\}e\textasciicircum\{-\textbackslash rho(t)\}\$. This \$f\_0\$ behaves like a normal density for small \$t\$, like an exponential density for large \$t\$. At least for me, this was rather surprising--I would have expected an \$f\_0\$ with much heavier tails. This result is a particular case of a more general one that can be stated roughly as follows: Assume that \$F\$ belongs to some convex set \$C\$ of distribution functions. Then the most robust (\$M\$)-estimator for the set \$C\$ coincides with the maximum likelihood estimator for the unique \$F\_0 \textbackslash varepsilon C\$ which has the smallest Fisher information number \$I(F) = \textbackslash int (f'/f)\textasciicircum 2f dt\$ among all \$F \textbackslash varepsilon C\$. Miscellaneous related problems will also be treated: the case of non-symmetric contaminating distributions; the most robust estimator for the model of indeterminacy \$\textbackslash sup\_t|F(t) - \textbackslash Phi(t)| \textbackslash leqq \textbackslash epsilon\$; robust estimation of a scale parameter; how to estimate location, if scale and \$\textbackslash epsilon\$ are unknown; numerical computation of the estimators; more general estimators, e.g., minimizing \$\textbackslash sum\_\{i {$<$} j\} \textbackslash rho(x\_i - T, x\_j - T)\$, where \$\textbackslash rho\$ is a function of two arguments. Questions of small sample size theory will not be touched in this paper.},
  file = {/Users/mikhail/Zotero/storage/6TM4UFFX/Huber - 1964 - Robust Estimation of a Location Parameter.pdf}
}

@article{INTERPOLATIONPREDICTIONNETWORKSIRREGULARLY2019,
  title = {{{INTERPOLATION-PREDICTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES}}},
  author = {Shukla, Satya Narayan and Marlin, Benjamin M},
  date = {2019},
  langid = {english},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/RBTESTYB/Shukla and Marlin - 2019 - INTERPOLATION-PREDICTION NETWORKS FOR IRREGULARLY .pdf}
}

@article{IntroductionROCAnalysis2006,
  title = {An Introduction to {{ROC}} Analysis},
  author = {Fawcett, Tom},
  date = {2006-06-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  series = {{{ROC Analysis}} in {{Pattern Recognition}}},
  volume = {27},
  number = {8},
  pages = {861--874},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2005.10.010},
  url = {https://www.sciencedirect.com/science/article/pii/S016786550500303X},
  urldate = {2024-09-30},
  abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
  keywords = {Classifier evaluation,Evaluation metrics,ROC analysis},
  file = {/Users/mikhail/Zotero/storage/JXD9YZLH/Fawcett - 2006 - An introduction to ROC analysis.pdf;/Users/mikhail/Zotero/storage/H5HISQI3/S016786550500303X.html}
}

@online{Labrador2023,
  title = {Labrador: {{Exploring}} the {{Limits}} of {{Masked Language Modeling}} for {{Laboratory Data}}},
  shorttitle = {Labrador},
  author = {Bellamy, David R. and Kumar, Bhawesh and Wang, Cindy and Beam, Andrew},
  date = {2023-12-09},
  eprint = {2312.11502},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.11502},
  url = {http://arxiv.org/abs/2312.11502},
  urldate = {2024-07-02},
  abstract = {In this work we introduce Labrador, a pre-trained Transformer model for laboratory data. Labrador and BERT were pre-trained on a corpus of 100 million lab test results from electronic health records (EHRs) and evaluated on various downstream outcome prediction tasks. Both models demonstrate mastery of the pre-training task but neither consistently outperform XGBoost on downstream supervised tasks. Our ablation studies reveal that transfer learning shows limited effectiveness for BERT and achieves marginal success with Labrador. We explore the reasons for the failure of transfer learning and suggest that the data generating process underlying each patient cannot be characterized sufficiently using labs alone, among other factors. We encourage future work to focus on joint modeling of multiple EHR data categories and to include tree-based baselines in their evaluations.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/Users/mikhail/Zotero/storage/4RHIIMDX/Bellamy et al. - 2023 - Labrador Exploring the Limits of Masked Language .pdf;/Users/mikhail/Zotero/storage/MPPVLI7Z/2312.html}
}

@article{LabTestReccomendation2020,
  title = {Development of an {{Artificial Intelligence}}–{{Based Automated Recommendation System}} for {{Clinical Laboratory Tests}}: {{Retrospective Analysis}} of the {{National Health Insurance Database}}},
  shorttitle = {Development of an {{Artificial Intelligence}}–{{Based Automated Recommendation System}} for {{Clinical Laboratory Tests}}},
  author = {Islam, Md Mohaimenul and Yang, Hsuan-Chia and Poly, Tahmina Nasrin and Li, Yu-Chuan Jack},
  date = {2020-11-18},
  journaltitle = {JMIR Medical Informatics},
  shortjournal = {JMIR Med Inform},
  volume = {8},
  number = {11},
  pages = {e24163},
  issn = {2291-9694},
  doi = {10.2196/24163},
  url = {https://medinform.jmir.org/2020/11/e24163},
  urldate = {2024-05-20},
  abstract = {Background               Laboratory tests are considered an essential part of patient safety as patients’ screening, diagnosis, and follow-up are solely based on laboratory tests. Diagnosis of patients could be wrong, missed, or delayed if laboratory tests are performed erroneously. However, recognizing the value of correct laboratory test ordering remains underestimated by policymakers and clinicians. Nowadays, artificial intelligence methods such as machine learning and deep learning (DL) have been extensively used as powerful tools for pattern recognition in large data sets. Therefore, developing an automated laboratory test recommendation tool using available data from electronic health records (EHRs) could support current clinical practice.                                         Objective               The objective of this study was to develop an artificial intelligence–based automated model that can provide laboratory tests recommendation based on simple variables available in EHRs.                                         Methods               A retrospective analysis of the National Health Insurance database between January 1, 2013, and December 31, 2013, was performed. We reviewed the record of all patients who visited the cardiology department at least once and were prescribed laboratory tests. The data set was split into training and testing sets (80:20) to develop the DL model. In the internal validation, 25\% of data were randomly selected from the training set to evaluate the performance of this model.                                         Results               We used the area under the receiver operating characteristic curve, precision, recall, and hamming loss as comparative measures. A total of 129,938 prescriptions were used in our model. The DL-based automated recommendation system for laboratory tests achieved a significantly higher area under the receiver operating characteristic curve (AUROCmacro and AUROCmicro of 0.76 and 0.87, respectively). Using a low cutoff, the model identified appropriate laboratory tests with 99\% sensitivity.                                         Conclusions               The developed artificial intelligence model based on DL exhibited good discriminative capability for predicting laboratory tests using routinely collected EHR data. Utilization of DL approaches can facilitate optimal laboratory test selection for patients, which may in turn improve patient safety. However, future study is recommended to assess the cost-effectiveness for implementing this model in real-world clinical settings.},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 3},
  file = {/Users/mikhail/Zotero/storage/MVVKDYNY/Islam et al. - 2020 - Development of an Artificial Intelligence–Based Au.pdf}
}

@article{LearningImbalancedData2009,
  title = {Learning from {{Imbalanced Data}}},
  author = {He, Haibo and Garcia, Edwardo A.},
  date = {2009-09},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {21},
  number = {9},
  pages = {1263--1284},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2008.239},
  url = {https://ieeexplore.ieee.org/document/5128907},
  urldate = {2024-09-30},
  abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {active learning,assessment metrics.,Availability,classification,cost-sensitive learning,Data analysis,Data engineering,Data security,Decision making,Finance,Imbalanced learning,IP networks,kernel-based learning,Knowledge representation,Large-scale systems,notion,sampling methods,Surveillance},
  file = {/Users/mikhail/Zotero/storage/MHA69BA3/He and Garcia - 2009 - Learning from Imbalanced Data.pdf;/Users/mikhail/Zotero/storage/4NMIKW88/5128907.html}
}

@article{Matplotlib,
  title = {Matplotlib: {{A 2D}} Graphics Environment},
  author = {Hunter, J. D.},
  date = {2007},
  journaltitle = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {90--95},
  publisher = {IEEE COMPUTER SOC},
  doi = {10.1109/MCSE.2007.55},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.}
}

@article{MedBERT2021,
  title = {Med-{{BERT}}: Pretrained Contextualized Embeddings on Large-Scale Structured Electronic Health Records for Disease Prediction},
  shorttitle = {Med-{{BERT}}},
  author = {Rasmy, Laila and Xiang, Yang and Xie, Ziqian and Tao, Cui and Zhi, Degui},
  date = {2021-05-20},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digit. Med.},
  volume = {4},
  number = {1},
  pages = {1--13},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-021-00455-y},
  url = {https://www.nature.com/articles/s41746-021-00455-y},
  urldate = {2024-06-19},
  abstract = {Deep learning (DL)-based predictive models from electronic health records (EHRs) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required by these models to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of BERT on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. Inspired by BERT, we propose Med-BERT, which adapts the BERT framework originally developed for the text domain to the structured EHR domain. Med-BERT is a contextualized embedding model pretrained on a structured EHR dataset of 28,490,650 patients. Fine-tuning experiments showed that Med-BERT substantially improves the prediction accuracy, boosting the area under the receiver operating characteristics curve (AUC) by 1.21–6.14\% in two disease prediction tasks from two clinical databases. In particular, pretrained Med-BERT obtains promising performances on tasks with small fine-tuning training sets and can boost the AUC by more than 20\% or obtain an AUC as high as a model trained on a training set ten times larger, compared with deep learning models without Med-BERT. We believe that Med-BERT will benefit disease prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare.},
  langid = {english},
  keywords = {Disease prevention,Experimental models of disease,Health care,notion},
  file = {/Users/mikhail/Zotero/storage/3CZ85UDL/Rasmy et al. - 2021 - Med-BERT pretrained contextualized embeddings on .pdf}
}

@online{MemoryEfficientAttention,
  title = {Self-Attention {{Does Not Need}} \${{O}}(N\textasciicircum 2)\$ {{Memory}}},
  author = {Rabe, Markus N. and Staats, Charles},
  date = {2022-10-10},
  eprint = {2112.05682},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2112.05682},
  url = {http://arxiv.org/abs/2112.05682},
  urldate = {2024-11-02},
  abstract = {We present a very simple algorithm for attention that requires \$O(1)\$ memory with respect to sequence length and an extension to self-attention that requires \$O(\textbackslash log n)\$ memory. This is in contrast with the frequently stated belief that self-attention requires \$O(n\textasciicircum 2)\$ memory. While the time complexity is still \$O(n\textasciicircum 2)\$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires \$O(\textbackslash sqrt\{n\})\$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/mikhail/Zotero/storage/WJHBVJHG/Rabe and Staats - 2022 - Self-attention Does Not Need $O(n^2)$ Memory.pdf;/Users/mikhail/Zotero/storage/4NHPGWBY/2112.html}
}

@inproceedings{MetaEHRLearn2020,
  title = {Learning to Select the Best Forecasting Tasks for Clinical Outcome Prediction},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Xue, Yuan and Du, Nan and Mottram, Anne and Seneviratne, Martin and Dai, Andrew M.},
  date = {2020-12-06},
  series = {{{NIPS}} '20},
  pages = {15031--15041},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {The paradigm of 'pretraining' from a set of relevant auxiliary tasks and then 'finetuning' on a target task has been successfully applied in many different domains. However, when the auxiliary tasks are abundant, with complex relationships to the target task, using domain knowledge or searching over all possible pretraining setups is inefficient and suboptimal. To address this challenge, we propose a method to automatically select from a large set of auxiliary tasks, which yields a representation most useful to the target task. In particular, we develop an efficient algorithm that uses automatic auxiliary task selection within a nested-loop meta-learning process. We have applied this algorithm to the task of clinical outcome predictions in electronic medical records, learning from a large number of self-supervised tasks related to forecasting patient trajectories. Experiments on a real clinical dataset demonstrate the superior predictive performance of our method compared to direct supervised learning, naive pretraining and simple multitask learning, in particular in low-data scenarios when the primary task has very few examples. With detailed ablation analysis, we further show that the selection rules are interpretable and able to generalize to unseen target tasks with new data.},
  isbn = {978-1-71382-954-6},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/I6D48N7X/MetaEHRLearn2020.pdf}
}

@dataset{MIMICIII,
  title = {{{MIMIC-III Clinical Database}}},
  author = {Johnson, Alistair and Pollard, Tom and Mark, Roger},
  date = {2015},
  publisher = {PhysioNet},
  doi = {10.13026/C2XW26},
  url = {https://physionet.org/content/mimiciii/1.4/},
  urldate = {2024-07-30},
  abstract = {MIMIC-III is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. The database includes information such as demographics, vital sign measurements made at the bedside (\textasciitilde 1 data point per hour), laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality (including post-hospital discharge). MIMIC supports a diverse range of analytic studies spanning epidemiology, clinical decision-rule improvement, and electronic tool development. It is notable for three factors: it is freely available to researchers worldwide; it encompasses a diverse and very large population of ICU patients; and it contains highly granular data, including vital signs, laboratory results, and medications.},
  version = {1.4},
  keywords = {notion}
}

@inproceedings{MIMICIIIICDnotes2021,
  title = {Clinical {{Outcome Prediction}} from {{Admission Notes}} Using {{Self-Supervised Knowledge Integration}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Van Aken, Betty and Papaioannou, Jens-Michalis and Mayrdorfer, Manuel and Budde, Klemens and Gers, Felix and Loeser, Alexander},
  date = {2021},
  pages = {881--893},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.eacl-main.75},
  url = {https://aclanthology.org/2021.eacl-main.75},
  urldate = {2024-06-19},
  abstract = {Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel admission to discharge task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose clinical outcome pretraining to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data.},
  eventtitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/L8PGNJ7S/Van Aken et al. - 2021 - Clinical Outcome Prediction from Admission Notes u.pdf}
}

@inproceedings{ModelingLongtermDependencies2022,
  title = {Modeling Long-Term Dependencies and Short-Term Correlations in Patient Journey Data with Temporal Attention Networks for Health Prediction},
  booktitle = {Proceedings of the 13th {{ACM International Conference}} on {{Bioinformatics}} , {{Computational Biology}} and {{Health Informatics}}},
  author = {Liu, Yuxi and Zhang, Zhenhao and Yepes, Antonio Jimeno and Salim, Flora D.},
  date = {2022-08-07},
  publisher = {ACM},
  doi = {10.1145/3535508.3545535},
  url = {http://dx.doi.org/10.1145/3535508.3545535},
  keywords = {notion},
  annotation = {Priority: 2},
  file = {/Users/mikhail/Zotero/storage/IRD4UFW5/Liu et al. - 2022 - Modeling long-term dependencies and short-term cor.pdf}
}

@book{ModernIntroductionProbability2005,
  title = {A Modern Introduction to Probability and Statistics: Understandig Why and How},
  shorttitle = {A Modern Introduction to Probability and Statistics},
  editor = {Dekking, Michel},
  date = {2005},
  series = {Springer Texts in Statistics},
  publisher = {Springer},
  location = {London [Heidelberg]},
  isbn = {978-1-85233-896-1},
  langid = {english},
  pagetotal = {486},
  file = {/Users/mikhail/Zotero/storage/5MD6KEHF/Modern_intro_probability_statistics_Dekking05.pdf}
}

@article{MultidimensionalPatientAcuity2022,
  title = {Multi-Dimensional Patient Acuity Estimation with Longitudinal {{EHR}} Tokenization and Flexible Transformer Networks},
  author = {Shickel, Benjamin and Silva, Brandon and Ozrazgat-Baslanti, Tezcan and Ren, Yuanfang and Khezeli, Kia and Guan, Ziyuan and Tighe, Patrick J. and Bihorac, Azra and Rashidi, Parisa},
  date = {2022-11-09},
  journaltitle = {Frontiers in Digital Health},
  shortjournal = {Front. Digit. Health},
  volume = {4},
  issn = {2673-253X},
  doi = {10.3389/fdgth.2022.1029191},
  url = {http://dx.doi.org/10.3389/fdgth.2022.1029191},
  abstract = {{$<$}jats:p{$>$}Transformer model architectures have revolutionized the natura l language processing (NLP) domain and continue to produce state-of-th e-art results in text-based applications. Prior to the emergence of tr ansformers, traditional NLP models such as recurrent and convolutional neural networks demonstrated promising utility for patient-level pred ictions and health forecasting from longitudinal datasets. However, to our knowledge only few studies have explored transformers for predict ing clinical outcomes from electronic health record (EHR) data, and in our estimation, none have adequately derived a health-specific tokeni zation scheme to fully capture the heterogeneity of EHR systems. In th is study, we propose a dynamic method for tokenizing both discrete and continuous patient data, and present a transformer-based classifier u tilizing a joint embedding space for integrating disparate temporal pa tient measurements. We demonstrate the feasibility of our clinical AI framework through multi-task ICU patient acuity estimation, where we s imultaneously predict six mortality and readmission outcomes. Our long itudinal EHR tokenization and transformer modeling approaches resulted in more accurate predictions compared with baseline machine learning models, which suggest opportunities for future multimodal data integra tions and algorithmic support tools using clinical transformer network s.{$<$}/jats:p{$>$}},
  keywords = {notion},
  annotation = {Priority: 1},
  file = {/Users/mikhail/Zotero/storage/654KYB2A/Shickel et al. - 2022 - Multi-dimensional patient acuity estimation with l.pdf}
}

@online{MultiphaseFinetuning2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018-05-23},
  eprint = {1801.06146},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1801.06146},
  url = {http://arxiv.org/abs/1801.06146},
  urldate = {2024-09-16},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/mikhail/Zotero/storage/KD2TVKVI/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Classification.pdf;/Users/mikhail/Zotero/storage/L2IEYYMB/1801.html}
}

@article{mvtsTransformer2020,
  title = {A {{Transformer-based Framework}} for {{Multivariate Time Series Representation Learning}}},
  author = {{George Zerveas} and {Srideepika Jayaraman} and {Dhaval Patel} and {A. Bhamidipaty} and {Carsten Eickhoff}},
  date = {2020-10-06},
  journaltitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  shortjournal = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  doi = {10.1145/3447548.3467401},
  url = {https://consensus.app/papers/transformerbased-framework-multivariate-time-series-zerveas/140c011ab4c65af7834c595e4376903e/},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/XFJRDQJX/George Zerveas et al. - 2020 - A Transformer-based Framework for Multivariate Tim.pdf}
}

@online{NonImagingMedicalData2022,
  title = {Non-{{Imaging Medical Data Synthesis}} for {{Trustworthy AI}}: {{A Comprehensive Survey}}},
  shorttitle = {Non-{{Imaging Medical Data Synthesis}} for {{Trustworthy AI}}},
  author = {Xing, Xiaodan and Wu, Huanjun and Wang, Lichao and Stenson, Iain and Yong, May and Del Ser, Javier and Walsh, Simon and Yang, Guang},
  date = {2022},
  doi = {10.48550/ARXIV.2209.09239},
  url = {https://arxiv.org/abs/2209.09239},
  urldate = {2024-05-19},
  abstract = {Data quality is the key factor for the development of trustworthy AI in healthcare. A large volume of curated datasets with controlled confounding factors can help improve the accuracy, robustness and privacy of downstream AI algorithms. However, access to good quality datasets is limited by the technical difficulty of data acquisition and large-scale sharing of healthcare data is hindered by strict ethical restrictions. Data synthesis algorithms, which generate data with a similar distribution as real clinical data, can serve as a potential solution to address the scarcity of good quality data during the development of trustworthy AI. However, state-of-the-art data synthesis algorithms, especially deep learning algorithms, focus more on imaging data while neglecting the synthesis of non-imaging healthcare data, including clinical measurements, medical signals and waveforms, and electronic healthcare records (EHRs). Thus, in this paper, we will review the synthesis algorithms, particularly for non-imaging medical data, with the aim of providing trustworthy AI in this domain. This tutorial-styled review paper will provide comprehensive descriptions of non-imaging medical data synthesis on aspects including algorithms, evaluations, limitations and future research directions.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),notion},
  file = {/Users/mikhail/Zotero/storage/YVX3HP2T/Xing et al. - 2022 - Non-Imaging Medical Data Synthesis for Trustworthy.pdf}
}

@inproceedings{ParameterDesignApproaches2023,
  title = {Parameter {{Design Approaches}} Based on {{AI Techniques}} for {{Transformer Neural Network Optimization}}},
  booktitle = {2023 3rd {{International Conference}} on {{Intelligent Technologies}} ({{CONIT}})},
  author = {Singh, Gurpreet and Chaudhary, Mansi and Singh, Jaspreet and Madaan, Nikita},
  date = {2023-06},
  pages = {1--6},
  doi = {10.1109/CONIT59222.2023.10205645},
  url = {https://ieeexplore.ieee.org/document/10205645},
  urldate = {2024-05-22},
  abstract = {The Transformer neural network is a magnificent monster of the deep learning world, demanding attention with its capacity to digest data sequences with unmatched efficacy. It has a towering self-attention mechanism and powerful feedforward layers. People are in awe of its skills as a result of its skill in multimodal activities, language interpretation, and picture processing. Certainly, artificial intelligence (AI) continues to advance more quickly. Therefore, the preparation, procedure, preservation, and commercialization of the energy infrastructure are hotspots for research approaches based on data-driven technologies AI. In several areas of picture analysis and evaluating, including self-driving automobiles, it performs remarkably well. A brief overview of the transformer neural network and the specified input define are presented in this research. Next, a transformer neural network design accompanying model for tasks involving natural language processing is outlined. Following that, papers relating to distinct transformer neural network types together with distinct techniques and approaches have been addressed here through multiple points of view of different scenarios. This study additionally outlines many transformer neural network applications. Finally, various artificial intelligence-based strategies for transformer design optimising and prospective directions are highlighted.},
  eventtitle = {2023 3rd {{International Conference}} on {{Intelligent Technologies}} ({{CONIT}})},
  keywords = {Application of Transform NN,Approaches for TNNs,Artificial intelligence,Artificial Intelligence Techniques for Transformer Design,Image processing,Natural language processing,Neural networks,notion,Task analysis,Transformer Neural Network,Transformer Neural Network Design,Transformers,Transforms},
  file = {/Users/mikhail/Zotero/storage/PG573Y9P/10205645.html}
}

@article{PhysioNet,
  title = {{{PhysioNet}}},
  author = {Goldberger, A. L. and Amaral, L. A. N. and Glass, L. and Hausdorff, J. M. and family=Ivanov, given=P. Ch., given-i=P{{Ch}} and Mark, R. G. and Mietus, J. E. and Moody, G. B. and Peng, C.-K. and Stanley, H. E.},
  year = {2000 (June 13)},
  journaltitle = {Circulation},
  volume = {101},
  number = {23},
  pages = {e215--e220}
}

@article{PrecisionRecallPlot2015,
  title = {The {{Precision-Recall Plot Is More Informative}} than the {{ROC Plot When Evaluating Binary Classifiers}} on {{Imbalanced Datasets}}},
  author = {Saito, Takaya and Rehmsmeier, Marc},
  date = {2015-03-04},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {10},
  number = {3},
  pages = {e0118432},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0118432},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432},
  urldate = {2024-09-30},
  abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
  langid = {english},
  keywords = {Bioinformatics,Caenorhabditis elegans,Exponential functions,Genome-wide association studies,Interpolation,Measurement,MicroRNAs,Support vector machines},
  file = {/Users/mikhail/Zotero/storage/42CZ5DDX/Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers o.pdf}
}

@thesis{PREDICTINGDETERIORATIONPATIENTS2018,
  title = {{{PREDICTING DETERIORATION OF PATIENTS IN THE INTENSIVE CARE UNIT}}},
  date = {2018},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/QZYR84L8/1530013271.pdf}
}

@article{PREDICTINGNEUROLOGICALOUTCOMES,
  title = {{{TOWARDS PREDICTING NEUROLOGICAL OUTCOMES OF ICU PATIENTS WITH ANEURYSMAL SUBARACHNOID HEMORRHAGE}}},
  author = {Cederlöf, Antti},
  abstract = {Aneurysmal subarachnoid hemorrhage (aSAH) is an uncummon but severe health condition that requires emergency treatment. Patients suffering from the condition have a high risk of developing a delayed stroke called delayed cerebral ischemia (DCI), which is a major cause of poor outcomes for the patients. Both aSAH and DCI have been the topic of several recent studies, but their dynamics are still unknown and there are no robust models for predicting the occurrence of a DCI event or the outcomes of aSAH. The aim of this thesis is to find out more about the conditions of aSAH and DCI, to explore and find biomarkers that are connected to the outcomes of the aSAH patients, and to lay the groundwork towards building a predictive model that could help clinicians for early prediction of the outcomes. With the large amount of data available, machine learning (ML) has recently been used in many healthcare applications. This study conducts an exploratory data analysis using ML approaches and a database of aSAH patients in an intensive care unit (ICU) including vitals, laboratory values, and neurological markers. The data analysis utilizes visualization and clustering methods to find the connections between the biomarker data and the neurological outcomes of the patients. The variables that evaluate the neurological outcome are the development of DCI events, and a neurological outcome grading of recovery after brain injury. The results of this study suggest that the features in the ICU database are indicative of the neurological outcomes of the patients. They are not predictive of the outcomes on their own, but their combinations can provide good separation between patients with good and bad recovery. However, with the features and analyses used in this thesis, there were no indications for the possible prediction of DCI. This exploratory study found possible predictive biomarkers that could be used in early prediction of aSAH patient recovery based on neurological outcomes. Predictors for DCI were not found in this study, so the mission of exploring the causes and indicators of the life-threatening event still continues.},
  langid = {english},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/TZ7XIPGR/Cederlöf - TOWARDS PREDICTING NEUROLOGICAL OUTCOMES OF ICU PA.pdf}
}

@article{PredictionOnsetCardiovascular2020,
  title = {Prediction of the Onset of Cardiovascular Diseases from Electronic Health Records Using Multi-Task Gated Recurrent Units},
  author = {{Fernando Andreotti} and {F. S. Heldt} and {Basel Abu-Jamous} and {Ming Li} and {A. Javer} and {Oliver Carr} and {Stojan Jovanovic} and {N. Lipunova} and {B. Irving} and {Rabia T. Khan} and {R. Dürichen}},
  date = {2020},
  journaltitle = {arXiv.org},
  keywords = {notion},
  annotation = {Priority: 4}
}

@article{PretrainingMedicalSurvey2023,
  title = {Pre-Training in {{Medical Data}}: {{A Survey}}},
  shorttitle = {Pre-Training in {{Medical Data}}},
  author = {Qiu, Yixuan and Lin, Feng and Chen, Weitong and Xu, Miao},
  date = {2023-04},
  journaltitle = {Machine Intelligence Research},
  shortjournal = {Mach. Intell. Res.},
  volume = {20},
  number = {2},
  pages = {147--179},
  issn = {2731-538X, 2731-5398},
  doi = {10.1007/s11633-022-1382-8},
  url = {https://link.springer.com/10.1007/s11633-022-1382-8},
  urldate = {2024-06-18},
  abstract = {Medical data refers to health-related information associated with regular patient care or as part of a clinical trial program. There are many categories of such data, such as clinical imaging data, bio-signal data, electronic health records (EHR), and multi-modality medical data. With the development of deep neural networks in the last decade, the emerging pre-training paradigm has become dominant in that it has significantly improved machine learning methods′ performance in a data-limited scenario. In recent years, studies of pre-training in the medical domain have achieved significant progress. To summarize these technology advancements, this work provides a comprehensive survey of recent advances for pre-training on several major types of medical data. In this survey, we summarize a large number of related publications and the existing benchmarking in the medical domain. Especially, the survey briefly describes how some pre-training methods are applied to or developed for medical data. From a data-driven perspective, we examine the extensive use of pre-training in many medical scenarios. Moreover, based on the summary of recent pre-training studies, we identify several challenges in this field to provide insights for future studies.},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 0},
  file = {/Users/mikhail/Zotero/storage/GX8LR4KJ/Qiu et al. - 2023 - Pre-training in Medical Data A Survey.pdf}
}

@article{ProbabilisticPredictionLaboratory2023,
  title = {Probabilistic {{Prediction}} of {{Laboratory Test Information Yield}}},
  author = {Jiang, Yixing and Lee, Andrew H. and Ni, Xiaoyuan and Corbin, Conor K. and Irvin, Jeremy A. and Ng, Andrew Y. and Chen, Jonathan H.},
  date = {2023},
  journaltitle = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
  shortjournal = {AMIA Annu Symp Proc},
  volume = {2023},
  pages = {1007--1016},
  issn = {1942-597X},
  abstract = {Low-yield repetitive laboratory diagnostics burden patients and inflate cost of care. In this study, we assess whether stability in repeated laboratory diagnostic measurements is predictable with uncertainty estimates using electronic health record data available before the diagnostic is ordered. We use probabilistic regression to predict a distribution of plausible values, allowing use-time customization for various definitions of "stability" given dynamic ranges and clinical scenarios. After converting distributions into "stability" scores, the models achieve a sensitivity of 29\% for white blood cells, 60\% for hemoglobin, 100\% for platelets, 54\% for potassium, 99\% for albumin and 35\% for creatinine for predicting stability at 90\% precision, suggesting those fractions of repetitive tests could be reduced with low risk of missing important changes. The findings demonstrate the feasibility of using electronic health record data to identify low-yield repetitive tests and offer personalized guidance for better usage of testing while ensuring high quality care.},
  langid = {english},
  keywords = {Modivation,notion,Similar},
  annotation = {Priority: 3}
}

@inproceedings{RAPT2021,
  title = {{{RAPT}}: {{Pre-training}} of {{Time-Aware Transformer}} for {{Learning Robust Healthcare Representation}}},
  shorttitle = {{{RAPT}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Ren, Houxing and Wang, Jingyuan and Zhao, Wayne Xin and Wu, Ning},
  date = {2021-08-14},
  pages = {3503--3511},
  publisher = {ACM},
  location = {Virtual Event Singapore},
  doi = {10.1145/3447548.3467069},
  url = {https://dl.acm.org/doi/10.1145/3447548.3467069},
  urldate = {2024-05-19},
  abstract = {With the development of electronic health records (EHRs), prenatal care examination records have become available for developing automatic prediction or diagnosis approaches with machine learning methods. In this paper, we study how to effectively learn representations applied to various downstream tasks for EHR data. Although several methods have been proposed in this direction, they usually adapt classic sequential models to solve one specific diagnosis task or address unique EHR data issues. This makes it difficult to reuse these existing methods for the early diagnosis of pregnancy complications or provide a general solution to address the series of health problems caused by pregnancy complications. In this paper, we propose a novel model RAPT, which stands for RepresentAtion by Pre-training time-aware Transformer. To associate pre-training and EHR data, we design an architecture that is suitable for both modeling EHR data and pre-training, namely time-aware Transformer. To handle various characteristics in EHR data, such as insufficiency, we carefully devise three pre-training tasks to handle data insufficiency, data incompleteness and short sequence problems, namely similarity prediction, masked prediction and reasonability check. In this way, our representations can capture various EHR data characteristics. Extensive experimental results for four downstream tasks have shown the effectiveness of the proposed approach. We also introduce sensitivity analysis to interpret the model and design an interface to show results and interpretation for doctors. Finally, we implement a diagnosis system for pregnancy complications based on our pre-training model. Doctors and pregnant women can benefit from the diagnosis system in early diagnosis of pregnancy complications.},
  eventtitle = {{{KDD}} '21: {{The}} 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-8332-5},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 0},
  file = {/Users/mikhail/Zotero/storage/2HVUSY5A/RAPT.pdf;/Users/mikhail/Zotero/storage/6E7YKZXU/Screenshot 2024-06-18 at 10.23.32.png;/Users/mikhail/Zotero/storage/7DP39BIS/Ren et al. - 2021 - RAPT Pre-training of Time-Aware Transformer for L.png;/Users/mikhail/Zotero/storage/EFCGACDA/Ren et al. - 2021 - RAPT Pre-training of Time-Aware Transformer for L.png;/Users/mikhail/Zotero/storage/JDCKJWYR/Screenshot 2024-06-18 at 10.17.24.png}
}

@online{RegressionbasedDeepLearningPredicts2023,
  title = {Regression-Based {{Deep-Learning}} Predicts Molecular Biomarkers from Pathology Slides},
  author = {Nahhas, Omar S. M. El and Loeffler, Chiara M. L. and Carrero, Zunamys I. and family=Treeck, given=Marko, prefix=van, useprefix=true and Kolbinger, Fiona R. and Hewitt, Katherine J. and Muti, Hannah S. and Graziani, Mara and Zeng, Qinghe and Calderaro, Julien and Ortiz-Brüchle, Nadina and Yuan, Tanwei and Hoffmeister, Michael and Brenner, Hermann and Brobeil, Alexander and Reis-Filho, Jorge S. and Kather, Jakob Nikolas},
  date = {2023},
  doi = {10.48550/ARXIV.2304.05153},
  url = {https://arxiv.org/abs/2304.05153},
  urldate = {2024-05-19},
  abstract = {Deep Learning (DL) can predict biomarkers from cancer histopathology. Several clinically approved applications use this technology. Most approaches, however, predict categorical labels, whereas biomarkers are often continuous measurements. We hypothesized that regression-based DL outperforms classification-based DL. Therefore, we developed and evaluated a new self-supervised attention-based weakly supervised regression method that predicts continuous biomarkers directly from images in 11,671 patients across nine cancer types. We tested our method for multiple clinically and biologically relevant biomarkers: homologous repair deficiency (HRD) score, a clinically used pan-cancer biomarker, as well as markers of key biological processes in the tumor microenvironment. Using regression significantly enhances the accuracy of biomarker prediction, while also improving the interpretability of the results over classification. In a large cohort of colorectal cancer patients, regression-based prediction scores provide a higher prognostic value than classification-based scores. Our open-source regression approach offers a promising alternative for continuous biomarker analysis in computational pathology.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,notion},
  file = {/Users/mikhail/Zotero/storage/K3VS5RMY/Nahhas et al. - 2023 - Regression-based Deep-Learning predicts molecular .pdf}
}

@article{RegressionTransformerEnables2023,
  title = {Regression {{Transformer}} Enables Concurrent Sequence Regression and Generation for Molecular Language Modelling},
  author = {Born, Jannis and Manica, Matteo},
  date = {2023-04-06},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {5},
  number = {4},
  pages = {432--444},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00639-z},
  url = {https://www.nature.com/articles/s42256-023-00639-z},
  urldate = {2024-05-16},
  abstract = {Abstract             Despite tremendous progress of generative models in the natural sciences, their controllability remains challenging. One fundamentally missing aspect of molecular or protein generative models is an inductive bias that can reflect continuous properties of interest. To that end, we propose the Regression Transformer (RT), a method that abstracts regression as a conditional sequence modelling problem. This introduces a new direction for multitask language models, seamlessly bridging sequence regression and conditional sequence generation. We demonstrate that, despite using a nominal-scale training objective, the RT matches or surpasses the performance of conventional regression models in property prediction of small molecules, proteins and chemical reactions. Critically, priming the same model with continuous properties yields a competitive conditional generative model that outperforms specialized approaches in a substructure-constrained, property-driven molecule generation benchmark. Our dichotomous approach is facilitated by an alternating training scheme that enables the model to decorate seed sequences on the basis of desired property constraints, for example, to optimize reaction yield. We expect that the RT’s capability to jointly tackle predictive and generative tasks in biochemistry can find applications in property-driven, local exploration of the chemical or protein space. Such multitask approaches will pave the road towards foundation models in materials design.},
  langid = {english},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/MKSMFISQ/Born and Manica - 2023 - Regression Transformer enables concurrent sequence.pdf}
}

@online{RETAIN2017,
  title = {{{RETAIN}}: {{An Interpretable Predictive Model}} for {{Healthcare}} Using {{Reverse Time Attention Mechanism}}},
  shorttitle = {{{RETAIN}}},
  author = {Choi, Edward and Bahadori, Mohammad Taha and Kulas, Joshua A. and Schuetz, Andy and Stewart, Walter F. and Sun, Jimeng},
  date = {2017-02-26},
  eprint = {1608.05745},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1608.05745},
  urldate = {2024-06-17},
  abstract = {Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {/Users/mikhail/Zotero/storage/2SUPQFGW/Choi et al. - 2017 - RETAIN An Interpretable Predictive Model for Heal.pdf;/Users/mikhail/Zotero/storage/WJEUDKHL/1608.html}
}

@article{RobustDataScaling2016,
  title = {A Robust Data Scaling Algorithm to Improve Classification Accuracies in Biomedical Data},
  author = {Cao, Xi Hang and Stojkovic, Ivan and Obradovic, Zoran},
  date = {2016-09-09},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {17},
  number = {1},
  pages = {359},
  issn = {1471-2105},
  doi = {10.1186/s12859-016-1236-x},
  url = {https://doi.org/10.1186/s12859-016-1236-x},
  urldate = {2024-09-30},
  abstract = {Machine learning models have been adapted in biomedical research and practice for knowledge discovery and decision support. While mainstream biomedical informatics research focuses on developing more accurate models, the importance of data preprocessing draws less attention. We propose the Generalized Logistic (GL) algorithm that scales data uniformly to an appropriate interval by learning a generalized logistic function to fit the empirical cumulative distribution function of the data. The GL algorithm is simple yet effective; it is intrinsically robust to outliers, so it is particularly suitable for diagnostic/classification models in clinical/medical applications where the number of samples is usually small; it scales the data in a nonlinear fashion, which leads to potential improvement in accuracy.},
  langid = {english},
  keywords = {Artificial Intelligence,Classification model,Data normalization,Data scaling,Empirical cumulative distribution function,Generalized logistic function,Outlier},
  file = {/Users/mikhail/Zotero/storage/C6YXD8PS/GL normalization.pdf}
}

@article{SAND2018,
  title = {Attend and {{Diagnose}}: {{Clinical Time Series Analysis Using Attention Models}}},
  shorttitle = {Attend and {{Diagnose}}},
  author = {Song, Huan and Rajan, Deepta and Thiagarajan, Jayaraman and Spanias, Andreas},
  date = {2018-04-29},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11635},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11635},
  urldate = {2024-05-17},
  abstract = {With widespread adoption of electronic health records, there is an increased emphasis for predictive models that can effectively deal with clinical time-series data. Powered by Recurrent Neural Network (RNN) architectures with Long Short-Term Memory (LSTM) units, deep neural networks have achieved state-of-the-art results in several clinical prediction tasks. Despite the success of RNN, its sequential nature prohibits parallelized computing, thus making it inefficient particularly when processing long sequences. Recently, architectures which are based solely on attention mechanisms have shown remarkable success in transduction tasks in NLP, while being computationally superior. In this paper, for the first time, we utilize attention models for clinical time-series modeling, thereby dispensing recurrence entirely. We develop the SAnD (Simply Attend and Diagnose) architecture, which employs a masked, self-attention mechanism, and uses positional encoding and dense interpolation strategies for incorporating temporal order. Furthermore, we develop a multi-task variant of SAnD to jointly infer models with multiple diagnosis tasks. Using the recent MIMIC-III benchmark datasets, we demonstrate that the proposed approach achieves state-of-the-art performance in all tasks, outperforming LSTM models and classical baselines with hand-engineered features.},
  keywords = {notion},
  annotation = {Priority: 3},
  file = {/Users/mikhail/Zotero/storage/VYTYFUKD/Song et al. - 2018 - Attend and Diagnose Clinical Time Series Analysis.pdf}
}

@online{SANSformers2021,
  title = {{{SANSformers}}: {{Self-Supervised Forecasting}} in {{Electronic Health Records}} with {{Attention-Free Models}}},
  shorttitle = {{{SANSformers}}},
  author = {Kumar, Yogesh and Ilin, Alexander and Salo, Henri and Kulathinal, Sangita and Leinonen, Maarit K. and Marttinen, Pekka},
  date = {2021},
  doi = {10.48550/ARXIV.2108.13672},
  url = {https://arxiv.org/abs/2108.13672},
  urldate = {2024-05-19},
  abstract = {Despite the proven effectiveness of Transformer neural networks across multiple domains, their performance with Electronic Health Records (EHR) can be nuanced. The unique, multidimensional sequential nature of EHR data can sometimes make even simple linear models with carefully engineered features more competitive. Thus, the advantages of Transformers, such as efficient transfer learning and improved scalability are not always fully exploited in EHR applications. Addressing these challenges, we introduce SANSformer, an attention-free sequential model designed with specific inductive biases to cater for the unique characteristics of EHR data. In this work, we aim to forecast the demand for healthcare services, by predicting the number of patient visits to healthcare facilities. The challenge amplifies when dealing with divergent patient subgroups, like those with rare diseases, which are characterized by unique health trajectories and are typically smaller in size. To address this, we employ a self-supervised pretraining strategy, Generative Summary Pretraining (GSP), which predicts future summary statistics based on past health records of a patient. Our models are pretrained on a health registry of nearly one million patients, then fine-tuned for specific subgroup prediction tasks, showcasing the potential to handle the multifaceted nature of EHR data. In evaluation, SANSformer consistently surpasses robust EHR baselines, with our GSP pretraining method notably amplifying model performance, particularly within smaller patient subgroups. Our results illuminate the promising potential of tailored attention-free models and self-supervised pretraining in refining healthcare utilization predictions across various patient demographics.},
  pubstate = {prepublished},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),notion},
  annotation = {Priority: 1},
  file = {/Users/mikhail/Zotero/storage/GCZ9TUEK/Kumar et al. - 2021 - SANSformers Self-Supervised Forecasting in Electr.pdf}
}

@online{ScientificSymbolicRegression2023,
  title = {A {{Transformer Model}} for {{Symbolic Regression}} towards {{Scientific Discovery}}},
  author = {Lalande, Florian and Matsubara, Yoshitomo and Chiba, Naoya and Taniai, Tatsunori and Igarashi, Ryo and Ushiku, Yoshitaka},
  date = {2023-12-13},
  eprint = {2312.04070},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.04070},
  urldate = {2024-05-17},
  abstract = {Symbolic Regression (SR) searches for mathematical expressions which best describe numerical datasets. This allows to circumvent interpretation issues inherent to artificial neural networks, but SR algorithms are often computationally expensive. This work proposes a new Transformer model aiming at Symbolic Regression particularly focused on its application for Scientific Discovery. We propose three encoder architectures with increasing flexibility but at the cost of column-permutation equivariance violation. Training results indicate that the most flexible architecture is required to prevent from overfitting. Once trained, we apply our best model to the SRSD datasets (Symbolic Regression for Scientific Discovery datasets) which yields state-of-the-art results using the normalized tree-based edit distance, at no extra computational cost.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/GACND727/Lalande et al. - 2023 - A Transformer Model for Symbolic Regression toward.pdf}
}

@article{Seaborn,
  title = {Seaborn: Statistical Data Visualization},
  author = {Waskom, Michael L.},
  date = {2021},
  journaltitle = {Journal of Open Source Software},
  volume = {6},
  number = {60},
  pages = {3021},
  publisher = {The Open Journal},
  doi = {10.21105/joss.03021},
  url = {https://doi.org/10.21105/joss.03021}
}

@inproceedings{SelfPacedLearning,
  title = {Self-Paced Learning for Latent Variable Models},
  booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
  author = {Kumar, M. Pawan and Packer, Benjamin and Koller, Daphne},
  date = {2010},
  series = {{{NIPS}}'10},
  pages = {1189--1197},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.},
  pagetotal = {9}
}

@article{SelfSupervisedContrastiveLearning2023,
  title = {Self-{{Supervised Contrastive Learning}} for {{Medical Time Series}}: {{A Systematic Review}}},
  shorttitle = {Self-{{Supervised Contrastive Learning}} for {{Medical Time Series}}},
  author = {Liu, Ziyu and Alavi, Azadeh and Li, Minyi and Zhang, Xiang},
  date = {2023-04-23},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {23},
  number = {9},
  pages = {4221},
  issn = {1424-8220},
  doi = {10.3390/s23094221},
  url = {https://www.mdpi.com/1424-8220/23/9/4221},
  urldate = {2024-05-19},
  abstract = {Medical time series are sequential data collected over time that measures health-related signals, such as electroencephalography (EEG), electrocardiography (ECG), and intensive care unit (ICU) readings. Analyzing medical time series and identifying the latent patterns and trends that lead to uncovering highly valuable insights for enhancing diagnosis, treatment, risk assessment, and disease progression. However, data mining in medical time series is heavily limited by the sample annotation which is time-consuming and labor-intensive, and expert-depending. To mitigate this challenge, the emerging self-supervised contrastive learning, which has shown great success since 2020, is a promising solution. Contrastive learning aims to learn representative embeddings by contrasting positive and negative samples without the requirement for explicit labels. Here, we conducted a systematic review of how contrastive learning alleviates the label scarcity in medical time series based on PRISMA standards. We searched the studies in five scientific databases (IEEE, ACM, Scopus, Google Scholar, and PubMed) and retrieved 1908 papers based on the inclusion criteria. After applying excluding criteria, and screening at title, abstract, and full text levels, we carefully reviewed 43 papers in this area. Specifically, this paper outlines the pipeline of contrastive learning, including pre-training, fine-tuning, and testing. We provide a comprehensive summary of the various augmentations applied to medical time series data, the architectures of pre-training encoders, the types of fine-tuning classifiers and clusters, and the popular contrastive loss functions. Moreover, we present an overview of the different data types used in medical time series, highlight the medical applications of interest, and provide a comprehensive table of 51 public datasets that have been utilized in this field. In addition, this paper will provide a discussion on the promising future scopes such as providing guidance for effective augmentation design, developing a unified framework for analyzing hierarchical time series, and investigating methods for processing multimodal data. Despite being in its early stages, self-supervised contrastive learning has shown great potential in overcoming the need for expert-created annotations in the research of medical time series.},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 4},
  file = {/Users/mikhail/Zotero/storage/QIAZBTP9/Liu et al. - 2023 - Self-Supervised Contrastive Learning for Medical T.pdf}
}

@inproceedings{SETOR2021,
  title = {Sequential {{Diagnosis Prediction}} with {{Transformer}} and {{Ontological Representation}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Peng, Xueping and Long, Guodong and Shen, Tao and Wang, Sen and Jiang, Jing},
  date = {2021-12},
  pages = {489--498},
  publisher = {IEEE},
  location = {Auckland, New Zealand},
  doi = {10.1109/ICDM51629.2021.00060},
  url = {https://ieeexplore.ieee.org/document/9679070/},
  urldate = {2024-05-18},
  eventtitle = {2021 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  isbn = {978-1-66542-398-4},
  keywords = {notion},
  annotation = {Priority: 4},
  file = {/Users/mikhail/Zotero/storage/KXVL8F4M/SETOR.pdf}
}

@article{SnakeMake,
  title = {Sustainable Data Analysis with {{Snakemake}} [Version 2; Peer Review: 2 Approved]},
  author = {Mölder, F and family=Jablonski, given=KP, given-i=KP and Letcher, B and family=Hall, given=MB, given-i=MB and family=Tomkins-Tinch, given=CH, given-i=CH and Sochat, V and Forster, J and Lee, S and family=Twardziok, given=SO, given-i=SO and Kanitz, A and Wilm, A and Holtgrewe, M and Rahmann, S and Nahnsen, S and K�ster, J},
  date = {2021},
  journaltitle = {F1000Research},
  volume = {10},
  number = {33},
  doi = {10.12688/f1000research.29032.2}
}

@manual{SparkRFrontEnd2024,
  type = {manual},
  title = {{{SparkR}}: {{R}} Front End for 'Apache Spark'},
  author = {{The Apache Software Foundation}},
  date = {2024},
  url = {https://www.apache.org https://spark.apache.org}
}

@article{STraTS2022,
  title = {Self-{{Supervised Transformer}} for {{Sparse}} and {{Irregularly Sampled Multivariate Clinical Time-Series}}},
  shorttitle = {{{STraTS}}},
  author = {Tipirneni, Sindhu and Reddy, Chandan K.},
  date = {2022-12-31},
  journaltitle = {ACM Transactions on Knowledge Discovery from Data},
  shortjournal = {ACM Trans. Knowl. Discov. Data},
  volume = {16},
  number = {6},
  pages = {1--17},
  issn = {1556-4681, 1556-472X},
  doi = {10.1145/3516367},
  url = {https://dl.acm.org/doi/10.1145/3516367},
  urldate = {2024-06-19},
  abstract = {Multivariate time-series data are frequently observed in critical care settings and are typically characterized by sparsity (missing information) and irregular time intervals. Existing approaches for learning representations in this domain handle these challenges by either aggregation or imputation of values, which in-turn suppresses the fine-grained information and adds undesirable noise/overhead into the machine learning model. To tackle this problem, we propose a               S               elf-supervised               Tra               nsformer for               T               ime-               S               eries (STraTS) model, which overcomes these pitfalls by treating time-series as a set of observation triplets instead of using the standard dense matrix representation. It employs a novel Continuous Value Embedding technique to encode continuous time and variable values without the need for discretization. It is composed of a Transformer component with multi-head attention layers, which enable it to learn contextual triplet embeddings while avoiding the problems of recurrence and vanishing gradients that occur in recurrent architectures. In addition, to tackle the problem of limited availability of labeled data (which is typically observed in many healthcare applications), STraTS utilizes self-supervision by leveraging unlabeled data to learn better representations by using time-series forecasting as an auxiliary proxy task. Experiments on real-world multivariate clinical time-series benchmark datasets demonstrate that STraTS has better prediction performance than state-of-the-art methods for mortality prediction, especially when labeled data is limited. Finally, we also present an interpretable version of STraTS, which can identify important measurements in the time-series data. Our data preprocessing and model implementation codes are available at               https://github.com/sindhura97/STraTS               .},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 0},
  file = {/Users/mikhail/Zotero/storage/7DC2D3FI/STRATS.pdf}
}

@article{SymFormerEndtoendSymbolic2022,
  title = {{{SymFormer}}: {{End-to-end}} Symbolic Regression Using Transformer-Based Architecture},
  author = {{Martin Vastl} and {Jonás Kulhánek} and {Jiří Kubalík} and {Erik Derner} and {R. Babuška}},
  date = {2022-05-31},
  journaltitle = {ArXiv},
  shortjournal = {ArXiv},
  volume = {abs/2205.15764},
  doi = {10.48550/arXiv.2205.15764},
  url = {https://consensus.app/papers/symformer-endtoend-regression-using-architecture-vastl/a3a2c1afe1515e1db172fcbc7c45d0b3/},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/6GSZ48HX/Martin Vastl et al. - 2022 - SymFormer End-to-end symbolic regression using tr.pdf}
}

@article{T3Net2021,
  title = {A Comparison of Attentional Neural Network Architectures for Modeling with Electronic Medical Records},
  author = {Finch, Anthony and Crowell, Alexander and Chang, Yung-Chieh and Parameshwarappa, Pooja and Martinez, Jose and Horberg, Michael},
  date = {2021-07-01},
  journaltitle = {JAMIA Open},
  volume = {4},
  number = {3},
  issn = {2574-2531},
  doi = {10.1093/jamiaopen/ooab064},
  url = {http://dx.doi.org/10.1093/jamiaopen/ooab064},
  abstract = {{$<$}jats:title{$>$}Abstract{$<$}/jats:title{$>$} {$<$}jats:sec{$>$} {$<$}jats:title{$>$}Objective{$<$}/jats:title{$>$} {$<$}jats:p{$>$}Attention networks learn an intelligent weig hted averaging mechanism over a series of entities, providing increase s to both performance and interpretability. In this article, we propos e a novel time-aware transformer-based network and compare it to anoth er leading model with similar characteristics. We also decompose model performance along several critical axes and examine which features co ntribute most to our model’s performance.{$<$}/jats:p{$>$} {$<$}/jats:sec{$>$} {$<$}jats:sec{$>$} {$<$}jats:title{$>$}Materials and methods{$<$}/jats:title{$>$} {$<$}jats:p{$>$}Using data sets representing patient records obtained between 2017 and 2019 by the Kaiser Permanente Mid-Atlantic States medical system, we construct four attentional models with varyi ng levels of complexity on two targets (patient mortality and hospital ization). We examine how incorporating transfer learning and demograph ic features contribute to model success. We also test the performance of a model proposed in recent medical modeling literature. We compare these models with out-of-sample data using the area under the receiver -operator characteristic (AUROC) curve and average precision as measur es of performance. We also analyze the attentional weights assigned by these models to patient diagnoses.{$<$}/jats:p{$>$} {$<$}/jats:sec{$>$} {$<$}jats:sec{$>$} {$<$}jats:title{$>$}Results{$<$}/jats:title{$>$} {$<$}jats:p{$>$}We found that our model significantly outper formed the alternative on a mortality prediction task (91.96\% AUROC ag ainst 73.82\% AUROC). Our model also outperformed on the hospitalizatio n task, although the models were significantly more competitive in tha t space (82.41\% AUROC against 80.33\% AUROC). Furthermore, we found tha t demographic features and transfer learning features which are freque ntly omitted from new models proposed in the EMR modeling space contri buted significantly to the success of our model.{$<$}/jats:p{$>$} {$<$}/jats:sec{$>$} {$<$}jats:sec{$>$} {$<$}jats:title{$>$}Discussion{$<$}/jats:title{$>$} {$<$}jats:p{$>$}We proposed an original construction of deep learning electronic medical record models which achieved very strong performance. We found that our unique model construction outperformed on several tasks in comparison to a leading literature alternative, ev en when input data was held constant between them. We obtained further improvements by incorporating several methods that are frequently ove rlooked in new model proposals, suggesting that it will be useful to e xplore these options further in the future.{$<$}/jats:p{$>$} {$<$}/jats:sec{$>$}},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 1},
  file = {/Users/mikhail/Zotero/storage/4X7VAQGZ/T3NET.pdf}
}

@online{T52019,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  date = {2019},
  doi = {10.48550/ARXIV.1910.10683},
  url = {https://arxiv.org/abs/1910.10683},
  urldate = {2024-06-26},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  pubstate = {prepublished},
  version = {4},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),notion},
  file = {/Users/mikhail/Zotero/storage/NB3AX2SV/Raffel et al. - 2019 - Exploring the Limits of Transfer Learning with a U.pdf}
}

@online{tcn,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  date = {2018-04-19},
  eprint = {1803.01271},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.01271},
  url = {http://arxiv.org/abs/1803.01271},
  urldate = {2024-08-26},
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/mikhail/Zotero/storage/3BP4RT7K/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf;/Users/mikhail/Zotero/storage/47ARYJRK/1803.html}
}

@article{TEE4EHR2024,
  title = {{{TEE4EHR}}: {{Transformer Event Encoder}} for {{Better Representation Learning}} in {{Electronic Health Records}}},
  shorttitle = {{{TEE4EHR}}},
  author = {Karami, Hojjat and Atienza, David and Ionescu, Anisoara},
  date = {2024-08},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {154},
  eprint = {2402.06367},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {102903},
  issn = {09333657},
  doi = {10.1016/j.artmed.2024.102903},
  url = {http://arxiv.org/abs/2402.06367},
  urldate = {2024-07-02},
  abstract = {Irregular sampling of time series in electronic health records (EHRs) is one of the main challenges for developing machine learning models. Additionally, the pattern of missing data in certain clinical variables is not at random but depends on the decisions of clinicians and the state of the patient. Point process is a mathematical framework for analyzing event sequence data that is consistent with irregular sampling patterns. Our model, TEE4EHR, is a transformer event encoder (TEE) with point process loss that encodes the pattern of laboratory tests in EHRs. The utility of our TEE has been investigated in a variety of benchmark event sequence datasets. Additionally, we conduct experiments on two real-world EHR databases to provide a more comprehensive evaluation of our model. Firstly, in a self-supervised learning approach, the TEE is jointly learned with an existing attention-based deep neural network which gives superior performance in negative log-likelihood and future event prediction. Besides, we propose an algorithm for aggregating attention weights that can reveal the interaction between the events. Secondly, we transfer and freeze the learned TEE to the downstream task for the outcome prediction, where it outperforms state-of-the-art models for handling irregularly sampled time series. Furthermore, our results demonstrate that our approach can improve representation learning in EHRs and can be useful for clinical prediction tasks.},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/Users/mikhail/Zotero/storage/VYYBZYAV/TEE4EHR.pdf;/Users/mikhail/Zotero/storage/P2IC5ACH/2402.html}
}

@article{TERTIAN2022,
  title = {{{TERTIAN}}: {{Clinical Endpoint Prediction}} in {{ICU}} via {{Time-Aware Transforme}} r-{{Based Hierarchical Attention Network}}},
  author = {An, Ying and Liu, Yang and Chen, Xianlai and Sheng, Yu},
  date = {2022-12-16},
  journaltitle = {Computational Intelligence and Neuroscience},
  shortjournal = {Computational Intelligence and Neuroscience},
  volume = {2022},
  pages = {1--13},
  issn = {1687-5273},
  doi = {10.1155/2022/4207940},
  url = {http://dx.doi.org/10.1155/2022/4207940},
  abstract = {{$<$}jats:p{$>$}Accurately predicting the clinical endpoint in ICU based on th e patient’s electronic medical records (EMRs) is essential for the tim ely treatment of critically ill patients and allocation of medical res ources. However, the patient’s EMRs usually consist of a large amount of heterogeneous multivariate time series data such as laboratory test s and vital signs, which are produced irregularly. Most existing metho ds fail to effectively model the time irregularity inherent in longitu dinal patient medical records and capture the interrelationships among different types of data. To tackle these limitations, we propose a no vel time-aware transformer-based hierarchical attention network (TERTI AN) for clinical endpoint prediction. In this model, a time-aware tran sformer is introduced to learn the personalized irregular temporal pat terns of medical events, and a hierarchical attention mechanism is dep loyed to get the accurate patient fusion representation by comprehensi vely mining the interactions and correlations among multiple types of medical data. We evaluate our model on the MIMIC-III dataset and MIMIC -IV dataset for the task of mortality prediction, and the results show that TERTIAN achieves higher performance than state-of-the-art approa ches.{$<$}/jats:p{$>$}},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 2},
  file = {/Users/mikhail/Zotero/storage/SU3UVBPU/An et al. - 2022 - TERTIAN Clinical Endpoint Prediction in ICU via T.pdf}
}

@software{thesiscode,
  title = {Mshavliuk/Thesis\_code},
  author = {Mikhail Shavliuk},
  url = {https://github.com/mshavliuk/thesis_code},
  urldate = {2024-08-26},
  abstract = {The source code for thesis experiments and data processing},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/V5BX8T8R/thesis_code.html}
}

@online{ThesisWriting2024,
  title = {Thesis {{Writing B}} 2024 {{Isokääntä}}: {{Resource}}: {{Evidence-based Writing}}: {{A Guide}} to {{Evaluating Sources}} in {{Your Field}} | {{TUNI Moodle}}},
  url = {https://moodle.tuni.fi/mod/resource/view.php?id=3163564},
  urldate = {2024-09-05},
  file = {/Users/mikhail/Zotero/storage/2M5SJ92I/view.html}
}

@article{TransformEHR2023,
  title = {{{TransformEHR}}: Transformer-Based Encoder-Decoder Generative Model to Enhance Prediction of Disease Outcomes Using Electronic Health Records},
  shorttitle = {{{TransformEHR}}},
  author = {Yang, Zhichao and Mitra, Avijit and Liu, Weisong and Berlowitz, Dan and Yu, Hong},
  date = {2023-11-29},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {14},
  number = {1},
  pages = {7857},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-43715-z},
  url = {https://www.nature.com/articles/s41467-023-43715-z},
  urldate = {2024-05-19},
  abstract = {Abstract                            Deep learning transformer-based models using longitudinal electronic health records (EHRs) have shown a great success in prediction of clinical diseases or outcomes. Pretraining on a large dataset can help such models map the input space better and boost their performance on relevant tasks through finetuning with limited data. In this study, we present TransformEHR, a generative encoder-decoder model with transformer that is pretrained using a new pretraining objective—predicting all diseases and outcomes of a patient at a future visit from previous visits. TransformEHR’s encoder-decoder framework, paired with the novel pretraining objective, helps it achieve the new state-of-the-art performance on multiple clinical prediction tasks. Comparing with the previous model, TransformEHR improves area under the precision–recall curve by 2\% (               p               \,{$<$}\,0.001) for pancreatic cancer onset and by 24\% (               p               \,=\,0.007) for intentional self-harm in patients with post-traumatic stress disorder. The high performance in predicting intentional self-harm shows the potential of TransformEHR in building effective clinical intervention systems. TransformEHR is also generalizable and can be easily finetuned for clinical prediction tasks with limited data.},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 0},
  file = {/Users/mikhail/Zotero/storage/C3RAX553/Yang et al. - 2023 - TransformEHR transformer-based encoder-decoder generative model to enhance prediction of disease ou.pdf}
}

@article{TransformerbasedPlanningSymbolic2023,
  title = {Transformer-Based {{Planning}} for {{Symbolic Regression}}},
  author = {{Parshin Shojaee} and {Kazem Meidani} and {A. Farimani} and {C. Reddy}},
  date = {2023-03-13},
  journaltitle = {ArXiv},
  shortjournal = {ArXiv},
  volume = {abs/2303.06833},
  doi = {10.48550/arXiv.2303.06833},
  url = {https://consensus.app/papers/transformerbased-planning-symbolic-regression-shojaee/fdb040a796ac547aa7d94feea3641e43/},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/VCJ74UYL/Parshin Shojaee et al. - 2023 - Transformer-based Planning for Symbolic Regression.pdf}
}

@article{TransformerModelsHealthcare2024,
  title = {Transformer {{Models}} in {{Healthcare}}: {{A Survey}} and {{Thematic Analysis}} of {{Potentials}}, {{Shortcomings}} and {{Risks}}},
  shorttitle = {Transformer {{Models}} in {{Healthcare}}},
  author = {Denecke, Kerstin and May, Richard and Rivera-Romero, Octavio},
  date = {2024},
  journaltitle = {Journal of Medical Systems},
  shortjournal = {J Med Syst},
  volume = {48},
  number = {1},
  pages = {23},
  issn = {0148-5598},
  doi = {10.1007/s10916-024-02043-5},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10874304/},
  urldate = {2024-05-18},
  abstract = {Large Language Models (LLMs) such as General Pretrained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT), which use transformer model architectures, have significantly advanced artificial intelligence and natural language processing. Recognized for their ability to capture associative relationships between words based on shared context, these models are poised to transform healthcare by improving diagnostic accuracy, tailoring treatment plans, and predicting patient outcomes. However, there are multiple risks and potentially unintended consequences associated with their use in healthcare applications. This study, conducted with 28 participants using a qualitative approach, explores the benefits, shortcomings, and risks of using transformer models in healthcare. It analyses responses to seven open-ended questions using a simplified thematic analysis. Our research reveals seven benefits, including improved operational efficiency, optimized processes and refined clinical documentation. Despite these benefits, there are significant concerns about the introduction of bias, auditability issues and privacy risks. Challenges include the need for specialized expertise, the emergence of ethical dilemmas and the potential reduction in the human element of patient care. For the medical profession, risks include the impact on employment, changes in the patient-doctor dynamic, and the need for extensive training in both system operation and data interpretation.},
  keywords = {notion,survey},
  annotation = {Priority: 4},
  file = {/Users/mikhail/Zotero/storage/IAZT4M9Z/Denecke et al. - 2024 - Transformer Models in Healthcare A Survey and The.pdf;/Users/mikhail/Zotero/storage/4FAI7HX5/reading.html}
}

@article{TransformersHealthcareSurvey2023,
  title = {Transformers in {{Healthcare}}: {{A Survey}}},
  shorttitle = {Transformers in {{Healthcare}}},
  author = {Nerella, Subhash and Bandyopadhyay, Sabyasachi and Zhang, Jiaqing and Contreras, Miguel and Siegel, Scott and Bumin, Aysegul and Silva, Brandon and Sena, Jessica and Shickel, Benjamin and Bihorac, Azra and Khezeli, Kia and Rashidi, Parisa},
  date = {2023-06-30},
  doi = {10.48550/arXiv.2307.00067},
  url = {http://arxiv.org/abs/2307.00067},
  urldate = {2024-05-18},
  abstract = {With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of data, including medical imaging, structured and unstructured Electronic Health Records (EHR), social media, physiological signals, and biomolecular sequences. Those models could help in clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. We identified relevant studies using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We also discuss the benefits and limitations of using transformers in healthcare and examine issues such as computational cost, model interpretability, fairness, alignment with human values, ethical implications, and environmental impact.},
  keywords = {notion,survey},
  annotation = {Priority: 0},
  file = {/Users/mikhail/Zotero/storage/F4AGX4F7/Nerella et al. - 2023 - Transformers in Healthcare A Survey.pdf;/Users/mikhail/Zotero/storage/VPQE7ED9/2307.html}
}

@inproceedings{TransformersMTR2021,
  title = {Transformer-Based {{Multi-target Regression}} on {{Electronic Health Records}} for {{Primordial Prevention}} of {{Cardiovascular Disease}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Poulain, Raphael and Gupta, Mehak and Foraker, Randi and Beheshti, Rahmatollah},
  date = {2021-12-09},
  pages = {726--731},
  publisher = {IEEE},
  location = {Houston, TX, USA},
  doi = {10.1109/BIBM52615.2021.9669441},
  url = {https://ieeexplore.ieee.org/document/9669441/},
  urldate = {2024-05-16},
  abstract = {Machine learning algorithms have been widely used to capture the static and temporal patterns within electronic health records (EHRs). While many studies focus on the (primary) prevention of diseases, primordial prevention (preventing the factors that are known to increase the risk of a disease occurring) is still widely under-investigated. In this study, we propose a multi-target regression model leveraging transformers to learn the bidirectional representations of EHR data and predict the future values of 11 major modifiable r isk f actors of cardiovascular disease (CVD). Inspired by the proven results of pre-training in natural language processing studies, we apply the same principles on EHR data, dividing the training of our model into two phases: pre-training and fine-tuning. W e u se t he finetuned transformer model in a “multi-target regression” theme. Following this theme, we combine the 11 disjoint prediction tasks by adding shared and target-specific l ayers t o t he m odel and jointly train the entire model. We evaluate the performance of our proposed method on a large publicly available EHR dataset. Through various experiments, we demonstrate that the proposed method obtains a significant improvement (12.6\% MAE on average across all 11 different outputs) over the baselines.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  isbn = {978-1-66540-126-5},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 1},
  file = {/Users/mikhail/Zotero/storage/ZTJDBILD/Poulain et al. - 2021 - Transformer-based Multi-target Regression on Elect.pdf}
}

@online{TransformersRegressionMixture2023,
  title = {Transformers Can Optimally Learn Regression Mixture Models},
  author = {Pathak, Reese and Sen, Rajat and Kong, Weihao and Das, Abhimanyu},
  date = {2023-11-14},
  eprint = {2311.08362},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2311.08362},
  url = {http://arxiv.org/abs/2311.08362},
  urldate = {2024-05-18},
  abstract = {Mixture models arise in many regression problems, but most methods have seen limited adoption partly due to these algorithms' highly-tailored and model-specific nature. On the other hand, transformers are flexible, neural sequence models that present the intriguing possibility of providing general-purpose prediction methods, even in this mixture setting. In this work, we investigate the hypothesis that transformers can learn an optimal predictor for mixtures of regressions. We construct a generative process for a mixture of linear regressions for which the decision-theoretic optimal procedure is given by data-driven exponential weights on a finite set of parameters. We observe that transformers achieve low mean-squared error on data generated via this process. By probing the transformer's output at inference time, we also show that transformers typically make predictions that are close to the optimal predictor. Our experiments also demonstrate that transformers can learn mixtures of regressions in a sample-efficient fashion and are somewhat robust to distribution shifts. We complement our experimental observations by proving constructively that the decision-theoretic optimal procedure is indeed implementable by a transformer.},
  pubstate = {prepublished},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/EQ95PR77/Pathak et al. - 2023 - Transformers can optimally learn regression mixtur.pdf;/Users/mikhail/Zotero/storage/TVGHUM2P/2311.html}
}

@article{TransformersStatisticiansProvable2023,
  title = {Transformers as {{Statisticians}}: {{Provable In-Context Learning}} with {{In-Context Algorithm Selection}}},
  author = {{Yu Bai} and {Fan Chen} and {Haiquan Wang} and {Caiming Xiong} and {Song Mei}},
  date = {2023-06-07},
  journaltitle = {ArXiv},
  shortjournal = {ArXiv},
  volume = {abs/2306.04637},
  doi = {10.48550/arXiv.2306.04637},
  url = {https://consensus.app/papers/transformers-statisticians-incontext-learning-bai/0ab1da03dcc55247b3ff1a58dc9c8d49/},
  keywords = {notion},
  file = {/Users/mikhail/Zotero/storage/9SHQMPHP/Yu Bai et al. - 2023 - Transformers as Statisticians Provable In-Context.pdf}
}

@online{UniHPF2022,
  title = {{{UniHPF}} : {{Universal Healthcare Predictive Framework}} with {{Zero Domain Knowledge}}},
  shorttitle = {{{UniHPF}}},
  author = {Hur, Kyunghoon and Oh, Jungwoo and Kim, Junu and Kim, Jiyoun and Lee, Min Jae and Cho, Eunbyeol and Moon, Seong-Eun and Kim, Young-Hak and Choi, Edward},
  date = {2022-11-15},
  eprint = {2211.08082},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.08082},
  url = {http://arxiv.org/abs/2211.08082},
  urldate = {2024-05-21},
  abstract = {Despite the abundance of Electronic Healthcare Records (EHR), its heterogeneity restricts the utilization of medical data in building predictive models. To address this challenge, we propose Universal Healthcare Predictive Framework (UniHPF), which requires no medical domain knowledge and minimal pre-processing for multiple prediction tasks. Experimental results demonstrate that UniHPF is capable of building large-scale EHR models that can process any form of medical data from distinct EHR systems. We believe that our findings can provide helpful insights for further research on the multi-source learning of EHRs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {/Users/mikhail/Zotero/storage/5I96VSUG/Hur et al. - 2022 - UniHPF  Universal Healthcare Predictive Framework.pdf;/Users/mikhail/Zotero/storage/WT9EJFB9/2211.html}
}

@article{UseElectronicHealth2021,
  title = {Use of {{Electronic Health Data}} for {{Disease Prediction}}: {{A Comprehensive Literature Review}}},
  shorttitle = {Use of {{Electronic Health Data}} for {{Disease Prediction}}},
  author = {Hossain, Md. Ekramul and Khan, Arif and Moni, Mohammad Ali and Uddin, Shahadat},
  date = {2021-03-01},
  journaltitle = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  shortjournal = {IEEE/ACM Trans. Comput. Biol. and Bioinf.},
  volume = {18},
  number = {2},
  pages = {745--758},
  issn = {1545-5963, 1557-9964, 2374-0043},
  doi = {10.1109/TCBB.2019.2937862},
  url = {https://ieeexplore.ieee.org/document/8815739/},
  urldate = {2024-05-17},
  abstract = {Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.},
  langid = {english},
  keywords = {notion},
  annotation = {Priority: 1},
  file = {/Users/mikhail/Zotero/storage/RALAPN3P/Hossain et al. - 2021 - Use of Electronic Health Data for Disease Predicti.pdf}
}

@article{UsingMachineLearning2016,
  title = {Using {{Machine Learning}} to {{Predict Laboratory Test Results}}},
  author = {Luo, Yuan and Szolovits, Peter and Dighe, Anand S and Baron, Jason M},
  date = {2016-06-01},
  journaltitle = {American Journal of Clinical Pathology},
  volume = {145},
  number = {6},
  pages = {778--788},
  issn = {0002-9173, 1943-7722},
  doi = {10.1093/ajcp/aqw064},
  url = {https://academic.oup.com/ajcp/article/145/6/778/2836697},
  urldate = {2024-05-20},
  abstract = {Abstract                            Objectives               While clinical laboratories report most test results as individual numbers, findings, or observations, clinical diagnosis usually relies on the results of multiple tests. Clinical decision support that integrates multiple elements of laboratory data could be highly useful in enhancing laboratory diagnosis.                                         Methods               Using the analyte ferritin in a proof of concept, we extracted clinical laboratory data from patient testing and applied a variety of machine-learning algorithms to predict ferritin test results using the results from other tests. We compared predicted with measured results and reviewed selected cases to assess the clinical value of predicted ferritin.                                         Results               We show that patient demographics and results of other laboratory tests can discriminate normal from abnormal ferritin results with a high degree of accuracy (area under the curve as high as 0.97, held-out test data). Case review indicated that predicted ferritin results may sometimes better reflect underlying iron status than measured ferritin.                                         Conclusions               These findings highlight the substantial informational redundancy present in patient test results and offer a potential foundation for a novel type of clinical decision support aimed at integrating, interpreting, and enhancing the diagnostic value of multianalyte sets of clinical laboratory test results.},
  langid = {english},
  keywords = {Motivation,notion,similar},
  annotation = {Priority: 1},
  file = {/Users/mikhail/Zotero/storage/8LZV5KPT/Luo et al. - 2016 - Using Machine Learning to Predict Laboratory Test .pdf}
}

@misc{wandb,
  title = {Experiment Tracking with Weights and Biases},
  author = {Biewald, Lukas},
  date = {2020},
  url = {https://www.wandb.com/}
}

@online{Weiss2004Mining,
  title = {Mining with Rarity: A Unifying Framework: {{ACM SIGKDD Explorations Newsletter}}: {{Vol}} 6, {{No}} 1},
  url = {https://dl.acm.org/doi/10.1145/1007730.1007734},
  urldate = {2024-11-11},
  file = {/Users/mikhail/Zotero/storage/ADS7XRGV/Mining with rarity a unifying framework ACM SIGKDD Explorations Newsletter Vol 6, No 1.pdf;/Users/mikhail/Zotero/storage/D6NHYU9M/1007730.html}
}

@thesis{WhatsMissingMachine2023,
  title = {What's {{Missing}} from {{Machine Learning}} for {{Medicine}}? {{New Methods}} for {{Causal Effect Estimation}} and {{Representation Learning}} from {{EHR Data}}},
  shorttitle = {What's {{Missing}} from {{Machine Learning}} for {{Medicine}}?},
  author = {{David Rémy Bellamy}},
  date = {2023-02-03},
  institution = {Harvard University},
  url = {https://www.proquest.com/openview/c66cd47ee37ad3d68452fffc9f919e7b},
  urldate = {2024-11-14},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  langid = {english},
  pagetotal = {193},
  file = {/Users/mikhail/Zotero/storage/3VMMLJ94/1.html}
}

@misc{xFormers,
  title = {{{xFormers}}: {{A}} Modular and Hackable {{Transformer}} Modelling Library},
  author = {Lefaudeux, Benjamin and Massa, Francisco and Liskovich, Diana and Xiong, Wenhan and Caggiano, Vittorio and Naren, Sean and Xu, Min and Hu, Jieru and Tintore, Marta and Zhang, Susan and Labatut, Patrick and Haziza, Daniel and Wehrstedt, Luca and Reizenstein, Jeremy and Sizov, Grigory},
  date = {2022},
  url = {https://github.com/facebookresearch/xformers}
}

@article{yang2020combining,
  title = {Combining Deep Learning with Token Selection for Patient Phenotyping from Electronic Health Records},
  author = {Yang, Zhen and Dehmer, Matthias and Yli-Harja, Olli and Emmert-Streib, Frank},
  date = {2020},
  journaltitle = {Scientific reports},
  volume = {10},
  number = {1},
  pages = {1432},
  publisher = {Nature Publishing Group UK London}
}

@article{yang2023threshold,
  title = {Threshold-Learned {{CNN}} for Multi-Label Text Classification of Electronic Health Records},
  author = {Yang, Zhen and Emmert-Streib, Frank},
  date = {2023},
  journaltitle = {IEEE access : practical innovations, open solutions},
  shortjournal = {IEEE Access},
  publisher = {IEEE}
}
