\chapter{Discussion}
\label{ch:discussion}

This chapter interprets the key findings from our experiments, positioning them within the broader context of machine learning for healthcare applications. We examine the impact of the implemented model adjustments on performance, efficiency, and robustness. Through comparative analysis with baseline models and exploration of class rebalancing, normalization techniques, and noise robustness, we assess how these modifications enhance the model's utility in clinical settings.


\section{Model Enhancements}

We extended the original analysis by evaluating the performance of the model in data fractions greater than \qty{50}{\percent}, which were not covered in the original paper. Our results confirm that the \citefield{STraTS2022}{shorttitle} model continues to outperform other baselines at these higher data levels, strengthening its predictive robustness.

Furthermore, by disabling weight decay for norm and bias parameters to adhere to best practices and allowing the model to train for an unlimited number of epochs until convergence, we achieved notable improvements (up to \qty{5}{\percent} on \gls{aucpr}) in predictive accuracy on smaller data fractions. These optimizations highlight the importance of refining training configurations to attain optimal model performance.

Improvements in computational efficiency enabled a more rapid experimentation process, allowing up to 30 times faster iterations and contributing to greater environmental sustainability through reduced energy consumption. The decrease in memory requirements also allowed us to explore larger model variants with up to 25 times the original parameter count, which would have been computationally prohibitive with the original implementation.
 Although these larger models were not included in this study, this improved efficiency and scalability underscore the model's applicability to real-world clinical settings, where data complexity and resource constraints often present challenges. By making the model more computationally accessible, these refinements hold promise for more widely applicable and adaptable healthcare predictive models as data availability and computational resources continue to grow.

\section{Balancing the Imbalances}

The findings of our experiments on class balancing underscore the challenges of addressing class imbalance in mortality prediction tasks, where the balance of model calibration and its discrimination ability is crucial.

Our results indicate that weighted loss functions, particularly with a batch size of \(16\), helped to maintain balanced model performance. However, when smaller batch sizes were combined with gradient clipping, there was a tendency to underestimate the positive class probability. This may stem from the frequent absence of positive samples in mini-batches and the reduced effect of class weighting, resulting in gradient steps biased toward predicting the negative class. Notably, this underestimation was not observed with a small batch size without gradient clipping, which showed a minimal discrepancy between the mean predicted risk and the true mortality rate. This suggests that gradient clipping, while effective in preventing exploding gradients, can inadvertently constrain positive samples updates in small batch settings, limiting the model’s ability to learn effectively from the minority class.


In contrast, the experiment with a larger batch size did not demonstrate the same behavior, supporting our hypothesis that gradient clipping has a more pronounced effect with smaller batch sizes and class weights. These findings highlight the importance of carefully selecting batch size and gradient clipping parameters to ensure effective learning from minority class examples.


The oversampling approach, while achieving better class separation, resulted in a consistent overestimation of the positive class probability, especially in smaller data fractions. This outcome suggests that oversampling can increase \gls{auroc} scores by improving discrimination, but at the same time negatively affect calibration, leading to overfitting and biasing predictions towards the minority class \cite[][13]{Weiss2004Mining}. Thus, while oversampling may yield better discrimination, it does not necessarily ensure well-calibrated predictions, especially when the model is used for clinical risk scores where accurate probability estimates are required.
% Additional calibration techniques, such as Platt scaling or isotonic regression, may be necessary to correct for these biases.

Despite these issues, the model maintains the ability to rank samples by their predicted probability of the positive class, as indicated by similar \gls{auroc} and \gls{aucpr} values across all experiments. However, the model's predictions are less calibrated with the actual mortality risk, highlighting the importance of evaluating the model's calibration in addition to its discrimination ability when assessing its performance as a risk score.

In summary, addressing class imbalance is complex, particularly when combined with gradient clipping, loss weighting, and small batch sizes. Weighted loss with a batch size of \(16\) provided a good balance of calibration and discrimination, suggesting that weighting alone may be an effective baseline method for similar tasks without introducing the overfitting risks seen with oversampling. Conversely, gradient clipping with small batch sizes may result in underestimating positive class probability, impairing the model’s learning from minority class examples. These findings confirm our initial hypothesis about the interplay between these hyperparameters and underscore the necessity of careful calibration to ensure accurate and reliable predictions.

\section{Impact of ECDF Normalization on Forecasting Performance}
\label{sec:forecasting_performance}

Our findings indicate that \gls{ecdf} normalization enhances the model's ability to predict the relative standing of values within a distribution (i.e., their percentiles), while maintaining comparable performance in the original domain. This approach is particularly beneficial in clinical contexts where the exact numeric value may be less important than its relative position within the population.

For example, in predicting glucose levels, a model using \gls{ecdf} normalization can more accurately place a glucose level prediction in the 95th percentile, which holds meaningful clinical implications. While it may be slightly less precise in terms of squared error in predicting specific high values, such as \qty{300}{\mg\per\deci\liter} and \qty{350}{\mg\per\deci\liter}, both values indicate extreme elevation, making the exact numerical distinction less clinically relevant.

These results suggest that by focusing on percentile ranks, \gls{ecdf} normalization can improve the model's performance in predicting clinically relevant metrics, especially when dealing with skewed data distributions and extreme values.

\section{Impact of ECDF Normalization on Classification Performance}

The fine-tuning metrics indicate minimal differences between the two normalization methods, with performance differing by no more than \qty{2}{\percent}. This finding suggests that \gls{ecdf} normalization does not substantially impact the model's ability to perform on the downstream mortality prediction task when applied to a sanitized dataset from which extreme values have been removed.

\section{Robustness to Noise and Outliers}

The noise robustness experiments reveal that while \gls{ecdf} normalization did not offer an advantage under Gaussian noise conditions, it demonstrated clear benefits when handling uniform noise, particularly in the presence of extreme outliers. In the Gaussian noise experiments (\cref{fig:gaussian_noise_results}), both \gls{ecdf} and Z-score normalization performed similarly, suggesting that Gaussian noise does not challenge model robustness in a way that differentiates between these normalization methods.

Originally, it was anticipated that \gls{ecdf} normalization would yield some improvements across all experiments, partly due to the inclusion of outliers that were initially filtered out. However, the results of the Gaussian noise experiment did not show significant benefits. This may be because the model already demonstrated robustness to Gaussian noise, rendering additional benefits from \gls{ecdf} normalization minimal. Another factor could be the relatively small number of outliers added to the data, which may not have been sufficient to significantly influence the model's performance. Since this dataset has been widely used and validated within the research community, receiving more than 7,700 citations, its extensive refinement could have contributed to the lack of observable improvement in model performance. The number of outliers in our dataset may be unrealistically low, given that the dataset has already undergone partial sanitization and validation, as indicated by the dataset changelog.\footnote{\url{https://physionet.org/content/mimiciii/1.4/}} Furthermore, the choice of selected features was likely influenced by the quality of the available data.

The comparable performance of both normalization methods in the \qty{100}{\percent} noise experiment indicates that the predictive capability in this scenario stems primarily from variables not subjected to noise, such as demographic data \(\mathbf{d}\), along with type \(f_i\), time \(t_i\), and the total number \(n\) of observations. This result aligns with our expectation that normalization methods would not influence the model's performance when all values are replaced with noise. Surprisingly, the observed magnitude of the performance drop was relatively small, indicating the resilience of the studied model to the chosen noise model. Nonetheless, this experiment provided a baseline for comparing the results of other noise experiments and estimating the model's predictive capacity in the absence of meaningful observation values \(v_i\). Notably, the observed performance was consistent with findings from a preliminary ablation study (not included in this work) in which the Value Embedding component was removed.


In contrast, under less extreme uniform noise conditions, \gls{ecdf} normalization consistently outperformed Z-score normalization across all performance metrics and data fractions (\cref{fig:uniform_noise_results}). This improved resilience suggests that \gls{ecdf} normalization is more robust in clinical settings where data can contain outliers due to errors or extreme measurements. One potential explanation for \gls{ecdf} normalization's advantage under uniform noise is its bounded range between \num{0} and \num{1}. As discussed earlier in \cref{sec:forecasting_performance}, this characteristic prevents extreme values from disproportionately affecting the model. In contrast, Z-score normalization can result in large, unbounded values for outliers, amplifying prediction errors. For example, on the noise-free test dataset, our best performing model trained with Z-score normalization resulted in approximately \qty{5.94}{\percent} of errors exceeding \num{1.0}, with some absolute errors reaching up to \num{63.86} and squared errors up to \num{4080.2442}.

The improved robustness to uniform noise observed with \gls{ecdf} normalization underscores its potential utility in real-world clinical data environments, where noise and outliers are common. Clinical datasets often contain extreme values due to variability in patient populations, measurement conditions, or rare medical events. The ability of \gls{ecdf} normalization to maintain predictive performance despite high levels of uniform noise suggests that it may help stabilize model predictions under these challenging conditions. This resilience has broader implications for future model design and development. Predictive models capable of handling both typical and extreme values without being disproportionately influenced by outliers are likely to be more reliable in clinical settings.

Furthermore, \gls{ecdf} normalization can reduce the need for outlier removal, which can be time-consuming and may compromise data integrity. Eliminating this preprocessing step can conserve resources and preserve the full range of data, allowing models to learn from the complete distribution. By preserving the existing outliers in the data and focusing on percentile ranks, \gls{ecdf} normalization enables models to learn from the full range of data, including extreme values, without the need for preprocessing steps that could introduce bias or data loss. This approach may also enhance scalability to larger datasets with many variables, which can be challenging to sanitize and preprocess manually.


In summary, our research of \gls{ecdf} normalization, class balancing, and model optimizations highlights promising avenues for building resilient, clinically applicable predictive models. The results emphasize the importance of normalization and processing choices in achieving robust and stable performance in healthcare datasets. In the following chapter, we consolidate these findings and reflect on the broader implications of this work within the field of healthcare machine learning.
